{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ddnn.nn import *\n",
    "from ddnn.validation import *\n",
    "from ddnn.data import *\n",
    "\n",
    "estimator = Estimator(\n",
    "    net=NeuralNetwork(\n",
    "        [\n",
    "            LinearLayer((17, 4)),\n",
    "            ActivationFunction(\"tanh\"),\n",
    "            LinearLayer((4, 1)),\n",
    "            ActivationFunction(\"logistic\"),\n",
    "        ]\n",
    "    ),\n",
    "    loss=LossFunction(\"MSE\"),\n",
    "    # optimizer=Optimizer(\"SGD\", learning_rate=1, momentum_coefficient=0.9, l2_coefficient=0),\n",
    "    optimizer=Optimizer(\"Adam\", learning_rate=1e-1, l2_coefficient=0),\n",
    "    batchsize=-1,\n",
    "    seed=123,\n",
    "    initializer=Initializer(\"glorot_uniform\"),\n",
    ")\n",
    "early_stopping = None\n",
    "epochs = 150\n",
    "dataset_type = (\"monk\", 1)\n",
    "# dataset_type = \"ML_cup\"\n",
    "log_every = 1\n",
    "losses = [\"MSE\", \"binary_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(dataset_type, tuple):\n",
    "    traindata = read_monks(dataset_type[1], \"train\")\n",
    "    traindata = onehot_encoding(data=traindata)\n",
    "\n",
    "    testdata = read_monks(dataset_type[1], \"test\")\n",
    "    testdata = onehot_encoding(data=testdata)\n",
    "else:\n",
    "    traindata, testdata = train_valid_split(read_ML_cup(\"train\"), seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traindata.shape, testdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlogger = Logger(\n",
    "    estimator,\n",
    "    losses=losses,\n",
    "    training_set=traindata,\n",
    "    validation_set=testdata,\n",
    "    every=log_every,\n",
    ")\n",
    "if early_stopping is not None:\n",
    "    print(\"early stopping\")\n",
    "    teststopper = TrainingThresholdStopping(estimator, early_stopping)\n",
    "\n",
    "    def callback(record):\n",
    "        testlogger(record)\n",
    "        teststopper(record)\n",
    "\n",
    "else:\n",
    "\n",
    "    def callback(record):\n",
    "        testlogger(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(traindata, callback=callback, n_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = estimator.evaluate(losses=losses, dataset=traindata)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = estimator.evaluate(losses=losses, dataset=testdata)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "\n",
    "# 1 plot with train and valid\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "@interact(\n",
    "    loss=testlogger._losses,\n",
    ")\n",
    "def plot_results(loss):\n",
    "    fig.tight_layout()\n",
    "    ax.clear()\n",
    "    for where, style in zip([\"train\", \"valid\"], [None, \"dotted\"]):\n",
    "        y = testlogger._scores[0][\"folds\"][0][where][loss]\n",
    "        if loss == \"binary_accuracy\":\n",
    "            # todo fix to show last not best\n",
    "            best = max(y)\n",
    "            form = \"{:.2}\"\n",
    "            logplot = False\n",
    "        else:\n",
    "            best = min(y)\n",
    "            form = \"{:.2E}\"\n",
    "            logplot = True\n",
    "        # scale to resemble number of epochs instead of plot points\n",
    "        ticks_x = ticker.FuncFormatter(\n",
    "            lambda x, pos: \"{0:g}\".format(x * testlogger._every)\n",
    "        )\n",
    "        ax.xaxis.set_major_formatter(ticks_x)\n",
    "        if logplot:\n",
    "            ax.set_yscale(\"log\")\n",
    "        else:\n",
    "            ax.set_yscale(\"linear\")\n",
    "        if where == \"valid\":\n",
    "            where = \"test\"\n",
    "        ax.plot(\n",
    "            y, label=f\"{where}: {form.format(best)}\", linestyle=style, color=\"black\"\n",
    "        )\n",
    "        ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "fixed_rng = np.random.default_rng(123)\n",
    "\n",
    "n_tries = 30\n",
    "train_loss_list = []\n",
    "test_loss_list = []\n",
    "train_loss_avg = {\"MSE\": 0, \"binary_accuracy\": 0}\n",
    "test_loss_avg = {\"MSE\": 0, \"binary_accuracy\": 0}\n",
    "train_loss_std = {}\n",
    "test_loss_std = {}\n",
    "for i in range(n_tries):\n",
    "    estimator.update_params(seed=fixed_rng.integers(0, sys.maxsize))\n",
    "    estimator.train(traindata, callback=lambda x: None, n_epochs=epochs)\n",
    "    train_res = estimator.evaluate(losses=losses, dataset=traindata)\n",
    "    test_res = estimator.evaluate(losses=losses, dataset=testdata)\n",
    "    print(train_res, test_res)\n",
    "    train_loss_list.append(train_res)\n",
    "    test_loss_list.append(test_res)\n",
    "    for loss in losses:\n",
    "        train_loss_avg[loss] += train_res[loss]\n",
    "        test_loss_avg[loss] += test_res[loss]\n",
    "\n",
    "for loss in losses:\n",
    "    train_loss_avg[loss] = train_loss_avg[loss] / n_tries\n",
    "    test_loss_avg[loss] = test_loss_avg[loss] / n_tries\n",
    "    train_loss_std[loss] = np.std([d[loss] for d in train_loss_list])\n",
    "    test_loss_std[loss] = np.std([d[loss] for d in test_loss_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train\", train_loss_avg, train_loss_std)\n",
    "print(\"test\", test_loss_avg, test_loss_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bd504499a3d325a7c1da9f8228712639636db49ae66a9009fa19a793144457f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
