{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a9880a-ceb7-4205-ba8f-9a4c4950322c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae4734e2-ca68-4204-9695-19ef2b9b896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    def __init__(self, fname=\"ReLU\"):\n",
    "        self._foward, self._backward = ActivationFunction.get_functions(fname)\n",
    "        self._buffer = None\n",
    "    \n",
    "    def foward(self, data):\n",
    "        if self._buffer:\n",
    "            print(\"No call to backward after previous foward call.\")\n",
    "        self._buffer = data\n",
    "        return self._foward(data)\n",
    "    \n",
    "    def backward(self, grad):\n",
    "        delta = grad * self._backward(self._buffer)\n",
    "        self._buffer = None\n",
    "        return delta\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_functions(fname):\n",
    "        if fname == \"ReLU\":\n",
    "            return (\n",
    "                lambda x: x*(x>0), # function\n",
    "                lambda x: 1*(x>0)  # gradient\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid Activation Function: {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "479b807f-a8d6-4e78-b5a5-a7df4b94b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    def __init__(self, fname=\"MSE\"):\n",
    "        self._foward, self._backward = LossFunction.get_functions(fname)\n",
    "        self._buffer = None\n",
    "        \n",
    "    def foward(self, pred, label):\n",
    "        if self._buffer:\n",
    "            print(\"No call to backward after previous foward call.\")\n",
    "        self._buffer = (pred, label)\n",
    "        return self._foward(pred, label)\n",
    "    \n",
    "    def backward(self):\n",
    "        delta = self._backward(*self._buffer)\n",
    "        self._buffer = None\n",
    "        return delta\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_functions(fname):\n",
    "        if fname == \"MSE\":\n",
    "            return (\n",
    "                lambda o, y: np.sum((o - y)**2) / (2*o.shape[0]), # function\n",
    "                lambda o, y: (o - y) / o.shape[0]                 # gradient\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid Activation Function: {fname}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b765a97e-d715-408a-bc8c-2e427430e831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, eta=1e-3, l2_coeff=0.01, alpha=0.3):\n",
    "        def optimize(old, grad, old_delta):\n",
    "            new_delta = -eta*grad + alpha*old_delta - 2*l2_coeff*old\n",
    "            return new_delta\n",
    "        self.optimize = optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cb46182-3db5-479a-9baf-50aecc06c2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer:\n",
    "    def __init__(self, shape):\n",
    "        self.weights = np.random.rand(*shape[::-1])\n",
    "        self.bias = np.random.rand(shape[1])\n",
    "        # gradient\n",
    "        self.weights_gradient = np.empty(shape[::-1])\n",
    "        self.bias_gradient = np.empty(shape[1])\n",
    "        # old update (momentum)\n",
    "        self.weights_delta = np.empty(shape[::-1])\n",
    "        self.bias_delta = np.empty(shape[1])\n",
    "        \n",
    "    def foward(self, data):\n",
    "        self._buffer = data\n",
    "        output = data @ self.weights.T + self.bias.T\n",
    "        return output\n",
    "    \n",
    "    def backward(self, output_gradient):\n",
    "        self.bias_gradient[:] = output_gradient.sum(axis=0)\n",
    "        self.weights_gradient[:] = output_gradient.T @ self._buffer\n",
    "        self._buffer = None\n",
    "        input_gradient = (output_gradient @ self.weights)\n",
    "        return input_gradient\n",
    "        \n",
    "    def update(self, weights_delta, bias_delta):\n",
    "        self.weights[:] = self.weights + weights_delta\n",
    "        self.bias[:] = self.bias + bias_delta\n",
    "        self.weights_delta[:] = weights_delta\n",
    "        self.bias_delta[:] = bias_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96a1536a-93e5-4378-9787-5fc43f153ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, net, loss=LossFunction(), optimizer=Optimizer()):\n",
    "        NeuralNetwork.check_network(net)\n",
    "        self.net = net\n",
    "        self.loss = loss\n",
    "        self.optimizer = optimizer\n",
    "        self._buffer = None\n",
    "        \n",
    "    def foward(self, data, label):\n",
    "        if self._buffer:\n",
    "            print(\"No call to backward after previous foward call.\")\n",
    "        out = data\n",
    "        for layer in self.net:\n",
    "            out = layer.foward(out)\n",
    "        return self.loss.foward(out, label), out\n",
    "    \n",
    "    def backward(self):\n",
    "        grad = self.loss.backward()\n",
    "        for layer in self.net[::-1]:\n",
    "            grad = layer.backward(grad)\n",
    "        return grad\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_network(net):\n",
    "        expected_layer_type = LinearLayer\n",
    "        for i, layer in enumerate(net):\n",
    "            if not isinstance(layer, expected_layer_type):\n",
    "                raise ValueError(f\"layer #{i} is of type {type(layer)} expected type is {expected_layer_type}\")\n",
    "            expected_layer_type = ActivationFunction if expected_layer_type == LinearLayer else LinearLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03cf90b-cfb5-47e2-9ead-5c8c1d8922d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8012ad8f-0cd6-4dbe-bca7-6fc01101cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tnet = torch.nn.Sequential(\n",
    "    torch.nn.Linear(8, 16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16, 16),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(16, 2)\n",
    ")\n",
    "tloss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f8218d-29a2-4195-9eff-b5fa0b4cd89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata = torch.rand(10, 8, requires_grad=True)\n",
    "tlabel = torch.rand(10, 2)\n",
    "tpred = tnet(tdata)\n",
    "terror = tloss(tpred, tlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f60b0a-f497-4e3d-9f4f-410d7ae50d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "terror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ada2283-e206-43ea-9067-d73bb1484af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991c7be-fc31-4973-a417-7e3ba466c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "terror.backward()\n",
    "tdata.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a273f7-a39e-446b-950c-e14106f05a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork([\n",
    "    LinearLayer((8,16)),\n",
    "    ActivationFunction(),\n",
    "    LinearLayer((16,16)),\n",
    "    ActivationFunction(),\n",
    "    LinearLayer((16, 2))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97699fdb-5434-49a9-aa3b-55bb052bc066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# force same weights for both networks\n",
    "for (layer, tlayer) in zip(net.net, tnet):\n",
    "    if isinstance(layer, LinearLayer):\n",
    "        layer.weights[:] = tlayer.weight.detach().numpy()\n",
    "        layer.bias[:] = tlayer.bias.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c38b5a6-04ce-4aba-b8a8-9ddc0fac408c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tdata.detach().numpy()\n",
    "label = tlabel.numpy()\n",
    "loss, pred = net.foward(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92db018-73ef-4dbb-b057-8b000319765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b7d2f0-bfb2-4011-bbd3-16fe93ad1019",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e14863-2099-4fb1-8b1a-141acb89ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = net.backward()\n",
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3f36a3-418b-4d39-ace5-2291f7bc99fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(grad, tdata.grad.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
