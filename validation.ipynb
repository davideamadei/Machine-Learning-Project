{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external libraries\n",
    "import numpy as np\n",
    "from typing import Iterator\n",
    "import itertools\n",
    "# local libraries\n",
    "from estimator import Estimator\n",
    "from util_classes import Dataset\n",
    "from optimizer import Optimizer\n",
    "from nn import NeuralNetwork, LinearLayer, ActivationFunction\n",
    "\n",
    "from loss import LossFunction\n",
    "\n",
    "# def filter_dict_by_key(dictionary: dict, keys: list[str]) -> dict:\n",
    "#     filtered_dict = {}\n",
    "#     for key in keys:\n",
    "#         if key in dictionary.keys():\n",
    "#             filtered_dict[key] = dictionary[key]\n",
    "#     return filtered_dict\n",
    "\n",
    "class GridSearch():\n",
    "    _net_keys = ['layers']\n",
    "    _optimizer_keys = ['eta', 'l2_coeff', 'alpha']\n",
    "    _loss_keys = ['fname']\n",
    "    _estimator_keys = ['batchsize']\n",
    "    #_global_keys = _net_keys + _optimizer_keys + _loss_keys + _estimator_keys\n",
    "\n",
    "    #dictionary containing translations from exposed names to names to pass to functions internally\n",
    "    _param_name_translations = {'layers': 'layers', 'l2': 'l2_coeff', 'momentum': 'alpha',\n",
    "        'eta': 'eta', 'loss': 'fname', 'batchsize': 'batchsize'}\n",
    "\n",
    "    #check param_grid and remove invalid values\n",
    "    #TODO implement\n",
    "    @staticmethod\n",
    "    def _check_param_grid(hyper_grid) -> bool:\n",
    "        return True\n",
    "\n",
    "    def __init__(self, estimator: Estimator, hyper_grid: dict):\n",
    "        if estimator == None or type(estimator) != Estimator:\n",
    "            raise TypeError\n",
    "        self._estimator = estimator\n",
    "        if hyper_grid == None or type(hyper_grid) != dict:\n",
    "            raise TypeError\n",
    "\n",
    "        #check for wrong values\n",
    "        if not GridSearch._check_param_grid(hyper_grid):\n",
    "            raise ValueError\n",
    "\n",
    "        #translate names of parameters and sort by key for better efficiency\n",
    "        new_grid = {}\n",
    "        for key in self. _param_name_translations.keys():\n",
    "            if key in hyper_grid:\n",
    "                new_grid[self._param_name_translations[key]] = hyper_grid[key]\n",
    "        self._hyper_grid = new_grid\n",
    "\n",
    "    #returns a list of data folds through indexes\n",
    "    def _generate_folds(self) -> Iterator[tuple[Dataset, Dataset]]:\n",
    "        n_folds = self._n_folds\n",
    "        dataset = self._dataset\n",
    "        data_size = dataset.ids.shape[0]\n",
    "        indices = np.arange(data_size)\n",
    "        \n",
    "        #TODO maybe shuffle not needed if we assume dataset has already been shuffled\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for index_lists in np.array_split(indices, n_folds):\n",
    "            #make mask to split test and training set indices\n",
    "            mask = np.zeros(data_size, dtype=bool)\n",
    "            mask[index_lists] = True\n",
    "            test_indices = indices[mask]\n",
    "            train_indices = indices[~mask]\n",
    "            #initialize test set and training set\n",
    "            test_set = Dataset(ids=dataset.ids[test_indices], labels=dataset.labels[test_indices], data=dataset.data[test_indices])\n",
    "            train_set = Dataset(ids=dataset.ids[train_indices], labels=dataset.labels[train_indices], data=dataset.data[train_indices])\n",
    "            yield (train_set, test_set)\n",
    "        \n",
    "\n",
    "    #returns the best set of hyperparameters\n",
    "    def k_fold(self, dataset: Dataset, n_folds: int):\n",
    "        if(isinstance(dataset, Dataset)):\n",
    "            self._dataset = dataset\n",
    "        else:\n",
    "            raise TypeError\n",
    "        \n",
    "        if(type(n_folds) == int):\n",
    "            self._n_folds = n_folds\n",
    "        else:\n",
    "            raise TypeError\n",
    "        \n",
    "        data_size = dataset.ids.shape[0]\n",
    "        if(n_folds > data_size):\n",
    "            raise ValueError\n",
    "        \n",
    "        hyper_grid = self._hyper_grid\n",
    "        estimator = self._estimator\n",
    "        input_dim = dataset.shape[1][0]\n",
    "        output_dim = dataset.shape[1][1]\n",
    "        \n",
    "        #check if output layer is correct for all combinations\n",
    "        for layers in hyper_grid['layers']:\n",
    "            if layers[-1][1] != output_dim:\n",
    "                raise ValueError('Number of units in last layer must be equal to the output dimension of the data')\n",
    "\n",
    "        #generates all combinations of hyperparameters\n",
    "        keys, values = zip(*hyper_grid.items())\n",
    "        param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "        #iterates all combinations of hyperparameters\n",
    "        for combination in param_combinations:\n",
    "            #filter parameters for various classes\n",
    "            loss_params = {key: combination[key] for key in self._loss_keys}\n",
    "            estimator_params = {key: combination[key] for key in self._estimator_keys}\n",
    "            optimizer_params = {key: combination[key] for key in self._optimizer_keys}\n",
    "            net_params = {key: combination[key] for key in self._net_keys}\n",
    "\n",
    "            #create dictionary of params to pass to constructors\n",
    "            estimator_params = {}\n",
    "            estimator_params['loss'] = LossFunction(**loss_params)\n",
    "            estimator_params['optimizer'] = Optimizer(**optimizer_params)\n",
    "            \n",
    "            #create list of layers to create NN\n",
    "            old_units = input_dim\n",
    "            layer_list = []\n",
    "            for layer in net_params['layers'][:-1]:\n",
    "                layer_list.append(LinearLayer((old_units, layer[0])))\n",
    "                #TODO maybe linear layers can be removed\n",
    "                layer_list.append(ActivationFunction(fname = layer[1]))\n",
    "                old_units = layer[0]\n",
    "            last_layer = net_params['layers'][-1]\n",
    "            layer_list.append(LinearLayer((old_units, last_layer[0])))\n",
    "            if last_layer[1] != 'linear':\n",
    "                layer_list.append(ActivationFunction(fname = layer_list[1]))\n",
    "            estimator_params['net'] = NeuralNetwork(layer_list)\n",
    "                \n",
    "\n",
    "            estimator.update_params(**estimator_params)\n",
    "\n",
    "            #iterates folds of dataset\n",
    "            for train_set, test_set in self._generate_folds():\n",
    "                tmp = 1\n",
    "        \n",
    "        \n",
    "        #for loop iterating all combinations\n",
    "            #give combination to estimator to initialize new NN\n",
    "            #train on TR set\n",
    "            #evaluate on VL set\n",
    "            #save results for combination\n",
    "        #return best combination\n",
    "            \n",
    "            \n",
    "    #returns an estimation of the risk for the model, average +- variance\n",
    "    def nested_k_fold(dataset: Dataset, inner_n_folds:int, outer_n_folds:int):\n",
    "        print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork([\n",
    "    LinearLayer((8, 16)),\n",
    "    ActivationFunction(),\n",
    "    LinearLayer((16, 16)),\n",
    "    ActivationFunction(),\n",
    "    LinearLayer((16, 2))\n",
    "])\n",
    "estimator = Estimator(net)\n",
    "grid = {}\n",
    "grid['eta'] = [0.1, 0.2, 0.3]\n",
    "grid['momentum'] = [0.1, 0.2, 0.3]\n",
    "grid['layers'] = [[(8, 'ReLU'),(16, 'ReLU'),(16,'ReLU'),(1,'linear')],\n",
    "                 [(7, 'ReLU'),(10, 'ReLU'),(1, 'linear')]]\n",
    "grid['loss'] = ['MSE']\n",
    "grid['l2'] = [0.1]\n",
    "grid['batchsize'] = [1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, (6, 1))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import read_monks\n",
    "data = read_monks(1, \"train\")\n",
    "ids = data.ids.copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = GridSearch(estimator, grid)\n",
    "selector.k_fold(data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "test = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "print(test)\n",
    "perm = np.random.permutation(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bd504499a3d325a7c1da9f8228712639636db49ae66a9009fa19a793144457f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
