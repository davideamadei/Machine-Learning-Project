{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external libraries\n",
    "import numpy as np\n",
    "from typing import Iterator\n",
    "import itertools\n",
    "# local libraries\n",
    "from estimator import Estimator\n",
    "from util_classes import Dataset\n",
    "from optimizer import Optimizer\n",
    "from nn import NeuralNetwork, LinearLayer, ActivationFunction\n",
    "\n",
    "from loss import LossFunction\n",
    "\n",
    "# def filter_dict_by_key(dictionary: dict, keys: list[str]) -> dict:\n",
    "#     filtered_dict = {}\n",
    "#     for key in keys:\n",
    "#         if key in dictionary.keys():\n",
    "#             filtered_dict[key] = dictionary[key]\n",
    "#     return filtered_dict\n",
    "\n",
    "class GridSearch():\n",
    "    _net_keys = ['layers']\n",
    "    _optimizer_keys = ['eta', 'l2_coeff', 'alpha']\n",
    "    _loss_keys = ['fname']\n",
    "    _estimator_keys = ['batchsize']\n",
    "    #_global_keys = _net_keys + _optimizer_keys + _loss_keys + _estimator_keys\n",
    "\n",
    "    #dictionary containing translations from exposed names to names to pass to functions internally\n",
    "    _param_name_translations = {'layers': 'layers', 'l2': 'l2_coeff', 'momentum': 'alpha',\n",
    "        'eta': 'eta', 'loss': 'fname', 'batchsize': 'batchsize'}\n",
    "\n",
    "    #check param_grid and remove invalid values\n",
    "    #TODO implement\n",
    "    @staticmethod\n",
    "    def _check_param_grid(hyper_grid) -> bool:\n",
    "        return True\n",
    "\n",
    "    def __init__(self, estimator: Estimator, hyper_grid: dict):\n",
    "        if estimator == None or type(estimator) != Estimator:\n",
    "            raise TypeError\n",
    "        self._estimator = estimator\n",
    "        if hyper_grid == None or type(hyper_grid) != dict:\n",
    "            raise TypeError\n",
    "\n",
    "        #check for wrong values\n",
    "        if not GridSearch._check_param_grid(hyper_grid):\n",
    "            raise ValueError\n",
    "\n",
    "        #translate names of parameters and sort by key for better efficiency\n",
    "        new_grid = {}\n",
    "        for key in self. _param_name_translations.keys():\n",
    "            if key in hyper_grid:\n",
    "                new_grid[self._param_name_translations[key]] = hyper_grid[key]\n",
    "        self._hyper_grid = new_grid\n",
    "\n",
    "    #returns a list of data folds through indexes\n",
    "    def _generate_folds(self) -> Iterator[tuple[Dataset, Dataset]]:\n",
    "        n_folds = self._n_folds\n",
    "        dataset = self._dataset\n",
    "        data_size = dataset.ids.shape[0]\n",
    "        indices = np.arange(data_size)\n",
    "        \n",
    "        #TODO maybe shuffle not needed if we assume dataset has already been shuffled\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for index_lists in np.array_split(indices, n_folds):\n",
    "            #make mask to split test and training set indices\n",
    "            mask = np.zeros(data_size, dtype=bool)\n",
    "            mask[index_lists] = True\n",
    "            test_indices = indices[mask]\n",
    "            train_indices = indices[~mask]\n",
    "            #initialize test set and training set\n",
    "            test_set = Dataset(ids=dataset.ids[test_indices], labels=dataset.labels[test_indices], data=dataset.data[test_indices])\n",
    "            train_set = Dataset(ids=dataset.ids[train_indices], labels=dataset.labels[train_indices], data=dataset.data[train_indices])\n",
    "            yield (train_set, test_set)\n",
    "        \n",
    "\n",
    "    #returns the best set of hyperparameters\n",
    "    def k_fold(self, dataset: Dataset, n_folds: int):\n",
    "        if(isinstance(dataset, Dataset)):\n",
    "            self._dataset = dataset\n",
    "        else:\n",
    "            raise TypeError\n",
    "        \n",
    "        if(type(n_folds) == int):\n",
    "            self._n_folds = n_folds\n",
    "        else:\n",
    "            raise TypeError\n",
    "        \n",
    "        data_size = dataset.ids.shape[0]\n",
    "        if(n_folds > data_size):\n",
    "            raise ValueError\n",
    "        \n",
    "        hyper_grid = self._hyper_grid\n",
    "        estimator = self._estimator\n",
    "        input_dim = dataset.shape[1][0]\n",
    "        output_dim = dataset.shape[1][1]\n",
    "        \n",
    "        #check if output layer is correct for all combinations\n",
    "        for layers in hyper_grid['layers']:\n",
    "            if layers[-1][0] != output_dim:\n",
    "                raise ValueError('Number of units in last layer must be equal to the output dimension of the data')\n",
    "\n",
    "        #generates all combinations of hyperparameters\n",
    "        keys, values = zip(*hyper_grid.items())\n",
    "        param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "        #iterates all combinations of hyperparameters\n",
    "        for combination in param_combinations:\n",
    "            #filter parameters for various classes\n",
    "            loss_params = {key: combination[key] for key in self._loss_keys}\n",
    "            estimator_params = {key: combination[key] for key in self._estimator_keys}\n",
    "            optimizer_params = {key: combination[key] for key in self._optimizer_keys}\n",
    "            net_params = {key: combination[key] for key in self._net_keys}\n",
    "\n",
    "            #create dictionary of params to pass to constructors\n",
    "            estimator_params['loss'] = LossFunction(**loss_params)\n",
    "            estimator_params['optimizer'] = Optimizer(**optimizer_params)\n",
    "            \n",
    "            #create list of layers to create NN\n",
    "            old_units = input_dim\n",
    "            layer_list = []\n",
    "            for layer in net_params['layers'][:-1]:\n",
    "                layer_list.append(LinearLayer((old_units, layer[0])))\n",
    "                #TODO maybe linear layers can be removed\n",
    "                layer_list.append(ActivationFunction(fname = layer[1]))\n",
    "                old_units = layer[0]\n",
    "            last_layer = net_params['layers'][-1]\n",
    "            layer_list.append(LinearLayer((old_units, last_layer[0])))\n",
    "            if last_layer[1] != 'linear':\n",
    "                layer_list.append(ActivationFunction(fname = layer_list[1]))\n",
    "            estimator_params['net'] = NeuralNetwork(layer_list)\n",
    "                \n",
    "            print(estimator_params)\n",
    "            estimator.update_params(**estimator_params)\n",
    "\n",
    "            #iterates folds of dataset\n",
    "            for train_set, test_set in self._generate_folds():\n",
    "                estimator.train(dataset = train_set, n_epochs=10)\n",
    "                print('ciao')\n",
    "        \n",
    "        \n",
    "        #for loop iterating all combinations\n",
    "            #give combination to estimator to initialize new NN\n",
    "            #train on TR set\n",
    "            #evaluate on VL set\n",
    "            #save results for combination\n",
    "        #return best combination\n",
    "            \n",
    "            \n",
    "    #returns an estimation of the risk for the model, average +- variance\n",
    "    def nested_k_fold(dataset: Dataset, inner_n_folds:int, outer_n_folds:int):\n",
    "        print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork([\n",
    "    LinearLayer((8, 16)),\n",
    "    ActivationFunction(),\n",
    "    LinearLayer((16, 16)),\n",
    "    ActivationFunction(),\n",
    "    LinearLayer((16, 2))\n",
    "])\n",
    "estimator = Estimator(net)\n",
    "grid = {}\n",
    "grid['eta'] = [0.1]\n",
    "grid['momentum'] = [0.1]\n",
    "grid['layers'] = [[(8, 'ReLU'),(1,'linear')]]\n",
    "                #  [(7, 'ReLU'),(10, 'ReLU'),(1, 'linear')]]\n",
    "grid['loss'] = ['MSE']\n",
    "grid['l2'] = [0.1]\n",
    "grid['batchsize'] = [124]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, (6, 1))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import read_monks\n",
    "data = read_monks(1, \"train\")\n",
    "ids = data.ids.copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batchsize': 124, 'loss': <loss.LossFunction object at 0x7f83db5393c0>, 'optimizer': <optimizer.Optimizer object at 0x7f83db53aef0>, 'net': <nn.NeuralNetwork object at 0x7f83db617190>}\n",
      "{'epoch': 1, 'loss': 163.07641016752498}\n",
      "{'epoch': 2, 'loss': 2.7698110601985393}\n",
      "{'epoch': 3, 'loss': 1.9168180104992487}\n",
      "{'epoch': 4, 'loss': 1.090201793822979}\n",
      "{'epoch': 5, 'loss': 0.652481206316557}\n",
      "{'epoch': 6, 'loss': 0.4351750651388183}\n",
      "{'epoch': 7, 'loss': 0.3248550010936393}\n",
      "{'epoch': 8, 'loss': 0.26642307354854666}\n",
      "{'epoch': 9, 'loss': 0.2340635911258158}\n",
      "{'epoch': 10, 'loss': 0.21541308994809535}\n",
      "ciao\n",
      "{'epoch': 11, 'loss': 0.19298060803294984}\n",
      "{'epoch': 12, 'loss': 0.18779849712627367}\n",
      "{'epoch': 13, 'loss': 0.18472753938936945}\n",
      "{'epoch': 14, 'loss': 0.18281252653736224}\n",
      "{'epoch': 15, 'loss': 0.18159778936906076}\n",
      "{'epoch': 16, 'loss': 0.1808217712276223}\n",
      "{'epoch': 17, 'loss': 0.18032421036353039}\n",
      "{'epoch': 18, 'loss': 0.1800045042305117}\n",
      "{'epoch': 19, 'loss': 0.1797988035662936}\n",
      "{'epoch': 20, 'loss': 0.1796663418320966}\n",
      "ciao\n",
      "{'epoch': 21, 'loss': 0.17958099583078513}\n",
      "{'epoch': 22, 'loss': 0.1795259873819793}\n",
      "{'epoch': 23, 'loss': 0.17949052444855476}\n",
      "{'epoch': 24, 'loss': 0.17946765878412285}\n",
      "{'epoch': 25, 'loss': 0.1794529141421329}\n",
      "{'epoch': 26, 'loss': 0.17944340565894357}\n",
      "{'epoch': 27, 'loss': 0.1794372736127712}\n",
      "{'epoch': 28, 'loss': 0.17943331893903183}\n",
      "{'epoch': 29, 'loss': 0.17943076845244282}\n",
      "{'epoch': 30, 'loss': 0.17942912355042234}\n",
      "ciao\n",
      "{'epoch': 31, 'loss': 0.1760439952610074}\n",
      "{'epoch': 32, 'loss': 0.17636694224671715}\n",
      "{'epoch': 33, 'loss': 0.17662613486316112}\n",
      "{'epoch': 34, 'loss': 0.17680147075409064}\n",
      "{'epoch': 35, 'loss': 0.17691592537327497}\n",
      "{'epoch': 36, 'loss': 0.17698999807168161}\n",
      "{'epoch': 37, 'loss': 0.17703782836567036}\n",
      "{'epoch': 38, 'loss': 0.17706869263869082}\n",
      "{'epoch': 39, 'loss': 0.17708860406619456}\n",
      "{'epoch': 40, 'loss': 0.177101448044661}\n",
      "ciao\n",
      "{'epoch': 41, 'loss': 0.18223605491812012}\n",
      "{'epoch': 42, 'loss': 0.18173013945725067}\n",
      "{'epoch': 43, 'loss': 0.18132616369312898}\n",
      "{'epoch': 44, 'loss': 0.1810541887905898}\n",
      "{'epoch': 45, 'loss': 0.1808772399476206}\n",
      "{'epoch': 46, 'loss': 0.18076297291404805}\n",
      "{'epoch': 47, 'loss': 0.18068929323380306}\n",
      "{'epoch': 48, 'loss': 0.18064179241438785}\n",
      "{'epoch': 49, 'loss': 0.18061116647163458}\n",
      "{'epoch': 50, 'loss': 0.18059141860464611}\n",
      "ciao\n"
     ]
    }
   ],
   "source": [
    "selector = GridSearch(estimator, grid)\n",
    "selector.k_fold(data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "test = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "print(test)\n",
    "perm = np.random.permutation(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bd504499a3d325a7c1da9f8228712639636db49ae66a9009fa19a793144457f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
