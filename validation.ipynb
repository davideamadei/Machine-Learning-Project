{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python libraries\n",
    "import numpy as np\n",
    "from typing import Iterator, Callable\n",
    "import itertools\n",
    "from numbers import Number\n",
    "\n",
    "# local libraries\n",
    "from estimator import Estimator\n",
    "from util_classes import Dataset\n",
    "from optimizer import Optimizer\n",
    "from nn import NeuralNetwork, LinearLayer, ActivationFunction\n",
    "\n",
    "from loss import LossFunction\n",
    "\n",
    "# def filter_dict_by_key(dictionary: dict, keys: list[str]) -> dict:\n",
    "#     filtered_dict = {}\n",
    "#     for key in keys:\n",
    "#         if key in dictionary.keys():\n",
    "#             filtered_dict[key] = dictionary[key]\n",
    "#     return filtered_dict\n",
    "\n",
    "\n",
    "class GridSearch:\n",
    "    _net_keys = [\"layers\"]\n",
    "    _optimizer_keys = [\"eta\", \"l2_coeff\", \"alpha\"]\n",
    "    _loss_keys = [\"fname\"]\n",
    "    _estimator_keys = [\"batchsize\"]\n",
    "    # _global_keys = _net_keys + _optimizer_keys + _loss_keys + _estimator_keys\n",
    "\n",
    "    # dictionary containing translations from exposed names to names to pass to functions internally\n",
    "    _param_name_translations = {\n",
    "        \"layers\": \"layers\",\n",
    "        \"l2\": \"l2_coeff\",\n",
    "        \"momentum\": \"alpha\",\n",
    "        \"eta\": \"eta\",\n",
    "        \"loss\": \"fname\",\n",
    "        \"batchsize\": \"batchsize\",\n",
    "    }\n",
    "\n",
    "    # check param_grid validity\n",
    "    @staticmethod\n",
    "    def _check_param_grid(param_dict, hyper_grid) -> bool:\n",
    "        activation_functions = [\"ReLU\", \"linear\"]\n",
    "        loss_functions = [\"MSE\"]\n",
    "\n",
    "        for key in param_dict.keys():\n",
    "            if not key in hyper_grid.keys():\n",
    "                raise ValueError(\n",
    "                    (\n",
    "                        \"All the following parameters must be present in the\"\n",
    "                        \" hyperparameter grid: \"\n",
    "                    ),\n",
    "                    list(param_dict.keys()),\n",
    "                )\n",
    "            if not isinstance(hyper_grid[key], list) or len(hyper_grid[key]) == 0:\n",
    "                raise ValueError(\n",
    "                    \"Each parameter must have an associated not empty list of\"\n",
    "                    \" parameters\"\n",
    "                )\n",
    "\n",
    "        # check layers\n",
    "        for net in hyper_grid[\"layers\"]:\n",
    "            for layer in net:\n",
    "                if (\n",
    "                    not isinstance(layer, tuple)\n",
    "                    or not len(layer) == 2\n",
    "                    or not isinstance(layer[0], int)\n",
    "                    or not isinstance(layer[1], str)\n",
    "                ):\n",
    "                    raise TypeError(\n",
    "                        (\n",
    "                            \"The layers parameter accepts a list of tuples of length 2\"\n",
    "                            \" with the first element being an integer\"\n",
    "                            \" that is the number of units in that layer, and the second\"\n",
    "                            \" element is a string containing the name of the activation\"\n",
    "                            \" function for that layer\"\n",
    "                        ),\n",
    "                    )\n",
    "                if layer[0] <= 0:\n",
    "                    raise ValueError(\"The number of units must be greater than 0\")\n",
    "                if not layer[1] in activation_functions:\n",
    "                    raise ValueError(\n",
    "                        (\n",
    "                            \"Only the following values are accepted for activation\"\n",
    "                            \" function: \"\n",
    "                        ),\n",
    "                        activation_functions,\n",
    "                    )\n",
    "\n",
    "        # check l2\n",
    "        for l2_coeff in hyper_grid[\"l2\"]:\n",
    "            if not isinstance(l2_coeff, Number):\n",
    "                raise TypeError(\"The L2 coefficient must be a number\")\n",
    "            if l2_coeff < 0:\n",
    "                raise ValueError(\"The L2 coefficient must be at least 0\")\n",
    "\n",
    "        # check momentum\n",
    "        for momentum in hyper_grid[\"momentum\"]:\n",
    "            if not isinstance(momentum, Number):\n",
    "                raise TypeError(\"The momentum parameter must be a number\")\n",
    "            if momentum < 0:\n",
    "                raise ValueError(\"The momentum parameter must be at least 0\")\n",
    "\n",
    "        # check eta\n",
    "        for eta in hyper_grid[\"eta\"]:\n",
    "            if not isinstance(eta, Number):\n",
    "                raise TypeError(\"The learning rate must be a number\")\n",
    "            if eta <= 0:\n",
    "                raise ValueError(\"The learning rate must be greater than 0\")\n",
    "\n",
    "        # check loss functions\n",
    "        for loss in hyper_grid[\"loss\"]:\n",
    "            if not isinstance(loss, str):\n",
    "                raise TypeError(\n",
    "                    \"The loss must be a string corresponding to the required loss\"\n",
    "                )\n",
    "            if loss not in loss_functions:\n",
    "                raise ValueError(\n",
    "                    \"Only the following values are accepted for the loss: \",\n",
    "                    loss_functions,\n",
    "                )\n",
    "\n",
    "        # check batchsize\n",
    "        for batchsize in hyper_grid[\"batchsize\"]:\n",
    "            if not isinstance(batchsize, int):\n",
    "                raise TypeError(\"The batch size must be an integer\")\n",
    "            if batchsize <= 0 and batchsize != -1:\n",
    "                raise ValueError(\n",
    "                    \"The batch size must be greater than 0. If -1 is passed as a value\"\n",
    "                    \" the size of the dataset will be used\"\n",
    "                )\n",
    "\n",
    "        return True\n",
    "\n",
    "    def __init__(self, estimator: Estimator, hyper_grid: dict):\n",
    "        if estimator == None or type(estimator) != Estimator:\n",
    "            raise TypeError\n",
    "        self._estimator = estimator\n",
    "        if hyper_grid == None or type(hyper_grid) != dict:\n",
    "            raise TypeError\n",
    "\n",
    "        # check for wrong values\n",
    "        GridSearch._check_param_grid(self._param_name_translations, hyper_grid)\n",
    "\n",
    "        # translate names of parameters and sort by key for better efficiency\n",
    "        new_grid = {}\n",
    "        for key in self._param_name_translations.keys():\n",
    "            if key in hyper_grid:\n",
    "                new_grid[self._param_name_translations[key]] = hyper_grid[key]\n",
    "        self._hyper_grid = new_grid\n",
    "\n",
    "    # returns a list of data folds through indexes\n",
    "    def _generate_folds(\n",
    "        self, dataset: Dataset, n_folds: int\n",
    "    ) -> Iterator[tuple[Dataset, Dataset]]:\n",
    "        data_size = dataset.ids.shape[0]\n",
    "        indices = np.arange(data_size)\n",
    "\n",
    "        # TODO maybe shuffle not needed if we assume dataset has already been shuffled\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        folds = []\n",
    "\n",
    "        for index_lists in np.array_split(indices, n_folds):\n",
    "            # make mask to split test and training set indices\n",
    "            mask = np.zeros(data_size, dtype=bool)\n",
    "            mask[index_lists] = True\n",
    "            test_indices = indices[mask]\n",
    "            train_indices = indices[~mask]\n",
    "            # initialize test set and training set\n",
    "            test_set = Dataset(\n",
    "                ids=dataset.ids[test_indices],\n",
    "                labels=dataset.labels[test_indices],\n",
    "                data=dataset.data[test_indices],\n",
    "            )\n",
    "            train_set = Dataset(\n",
    "                ids=dataset.ids[train_indices],\n",
    "                labels=dataset.labels[train_indices],\n",
    "                data=dataset.data[train_indices],\n",
    "            )\n",
    "            folds.append((train_set, test_set))\n",
    "        return folds\n",
    "\n",
    "    def _create_estimator_params(self, combination: dict, input_dim: int) -> dict:\n",
    "        # filter parameters for various classes\n",
    "        loss_params = {key: combination[key] for key in self._loss_keys}\n",
    "        estimator_params = {key: combination[key] for key in self._estimator_keys}\n",
    "        optimizer_params = {key: combination[key] for key in self._optimizer_keys}\n",
    "        net_params = {key: combination[key] for key in self._net_keys}\n",
    "\n",
    "        # create dictionary of params to pass to constructors\n",
    "        estimator_params[\"loss\"] = LossFunction(**loss_params)\n",
    "        estimator_params[\"optimizer\"] = Optimizer(**optimizer_params)\n",
    "\n",
    "        # create list of layers to create NN\n",
    "        old_units = input_dim\n",
    "        layer_list = []\n",
    "        for layer in net_params[\"layers\"][:-1]:\n",
    "            layer_list.append(LinearLayer((old_units, layer[0])))\n",
    "            # TODO maybe linear layers can be removed\n",
    "            layer_list.append(ActivationFunction(fname=layer[1]))\n",
    "            old_units = layer[0]\n",
    "        last_layer = net_params[\"layers\"][-1]\n",
    "        layer_list.append(LinearLayer((old_units, last_layer[0])))\n",
    "        if last_layer[1] != \"linear\":\n",
    "            layer_list.append(ActivationFunction(fname=layer_list[1]))\n",
    "        estimator_params[\"net\"] = NeuralNetwork(layer_list)\n",
    "        return estimator_params\n",
    "\n",
    "    # returns the best set of hyperparameters\n",
    "    def k_fold(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        n_folds: int,\n",
    "        n_epochs: int,\n",
    "        callback: Callable[[dict], None] = print,\n",
    "    ) -> dict:\n",
    "        hyper_grid = self._hyper_grid\n",
    "        estimator = self._estimator\n",
    "\n",
    "        data_size = dataset.shape[0]\n",
    "        input_dim = dataset.shape[1][0]\n",
    "        output_dim = dataset.shape[1][1]\n",
    "\n",
    "        # check n_folds value\n",
    "        if n_folds > data_size:\n",
    "            raise ValueError(\n",
    "                \"The number of folds cannot be greater than the number of samples in\"\n",
    "                \" the dataset\"\n",
    "            )\n",
    "        # check if output layer is correct for all combinations\n",
    "        for layers in hyper_grid[\"layers\"]:\n",
    "            if layers[-1][0] != output_dim:\n",
    "                raise ValueError(\n",
    "                    \"Number of units in last layer must be equal to the output\"\n",
    "                    \" dimension of the data\"\n",
    "                )\n",
    "        # check values for batchsize\n",
    "        for batchsize in hyper_grid[\"batchsize\"]:\n",
    "            if batchsize > data_size:\n",
    "                raise ValueError(\n",
    "                    \"The batchsize cannot be greater than the number of samples\"\n",
    "                )\n",
    "\n",
    "        folds = self._generate_folds(dataset=dataset, n_folds=n_folds)\n",
    "\n",
    "        # generates all combinations of hyperparameters\n",
    "        keys, values = zip(*hyper_grid.items())\n",
    "        param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "        combination_loss_list = []\n",
    "\n",
    "        # iterates all combinations of hyperparameters\n",
    "        for combination in param_combinations:\n",
    "            estimator_params = self._create_estimator_params(combination, input_dim)\n",
    "            if estimator_params[\"batchsize\"] == -1:\n",
    "                estimator_params[\"batchsize\"] = data_size\n",
    "            estimator.update_params(**estimator_params)\n",
    "\n",
    "            test_loss_list = []\n",
    "\n",
    "            print(combination)\n",
    "\n",
    "            # iterates folds of dataset\n",
    "            for train_set, test_set in folds:\n",
    "                estimator.train(dataset=train_set, n_epochs=n_epochs, callback=callback)\n",
    "                test_loss_list.append(estimator.evaluate(test_set))\n",
    "                estimator.reset()\n",
    "\n",
    "            test_loss_avg = sum(test_loss_list) / n_folds\n",
    "            test_loss_std = np.std(test_loss_list)\n",
    "\n",
    "            combination_loss_list.append(\n",
    "                {\n",
    "                    \"parameters\": combination,\n",
    "                    \"test_loss_avg\": test_loss_avg,\n",
    "                    \"test_loss_std\": test_loss_std,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        combination_loss_list.sort(key=lambda x: x[\"test_loss_avg\"])\n",
    "        return combination_loss_list\n",
    "\n",
    "    # returns an estimation of the risk for the model, average +- standard deviation\n",
    "    def nested_k_fold(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        inner_n_folds: int,\n",
    "        outer_n_folds: int,\n",
    "        n_epochs: int,\n",
    "        inner_callback: Callable[[dict], None] = print,\n",
    "        outer_callback: Callable[[dict], None] = print,\n",
    "    ) -> dict:\n",
    "        estimator = self._estimator\n",
    "        data_size = dataset.shape[0]\n",
    "        folds = self._generate_folds(dataset=dataset, n_folds=outer_n_folds)\n",
    "        input_dim = dataset.shape[1][0]\n",
    "\n",
    "        # check outer_n_folds value\n",
    "        if outer_n_folds > data_size:\n",
    "            raise ValueError(\n",
    "                \"The number of folds cannot be greater than the number of samples in\"\n",
    "                \" the dataset\"\n",
    "            )\n",
    "\n",
    "        test_loss_list = []\n",
    "\n",
    "        for train_set, test_set in folds:\n",
    "            train_results = self.k_fold(\n",
    "                dataset=train_set,\n",
    "                n_folds=inner_n_folds,\n",
    "                n_epochs=n_epochs,\n",
    "                callback=inner_callback,\n",
    "            )\n",
    "            params = train_results[0][\"parameters\"]\n",
    "            estimator_params = self._create_estimator_params(params, input_dim)\n",
    "            if estimator_params[\"batchsize\"] == -1:\n",
    "                estimator_params[\"batchsize\"] = data_size\n",
    "\n",
    "            estimator.update_params(**estimator_params)\n",
    "            estimator.train(\n",
    "                dataset=train_set, n_epochs=n_epochs, callback=outer_callback\n",
    "            )\n",
    "            test_loss_list.append(estimator.evaluate(test_set))\n",
    "\n",
    "        test_loss_avg = sum(test_loss_list) / outer_n_folds\n",
    "        test_loss_std = np.std(test_loss_list)\n",
    "\n",
    "        results = {\n",
    "            \"test_loss_list\": test_loss_list,\n",
    "            \"test_loss_avg\": test_loss_avg,\n",
    "            \"test_loss_std\": test_loss_std,\n",
    "        }\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork(\n",
    "    [\n",
    "        LinearLayer((8, 16)),\n",
    "        ActivationFunction(),\n",
    "        LinearLayer((16, 16)),\n",
    "        ActivationFunction(),\n",
    "        LinearLayer((16, 2)),\n",
    "    ]\n",
    ")\n",
    "estimator = Estimator(net)\n",
    "grid = {}\n",
    "grid[\"eta\"] = [0.001, 0.002]\n",
    "grid[\"momentum\"] = [0.1, 0.2]\n",
    "grid[\"layers\"] = [[(8, \"ReLU\"), (1, \"linear\")], [(4, \"ReLU\"), (1, \"linear\")]]\n",
    "grid[\"loss\"] = [\"MSE\"]\n",
    "grid[\"l2\"] = [0.1, 0.2]\n",
    "grid[\"batchsize\"] = [-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import read_monks, read_ML_cup\n",
    "\n",
    "# data = read_ML_cup(\"train\")\n",
    "data = read_monks(1, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = GridSearch(estimator, grid)\n",
    "selector.k_fold(data, 5, n_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.nested_k_fold(data, 5, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bd504499a3d325a7c1da9f8228712639636db49ae66a9009fa19a793144457f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
