{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO maybe change ValueError exceptions to show wrong value\n",
    "\n",
    "# python libraries\n",
    "import numpy as np\n",
    "from typing import Iterator, Callable\n",
    "import itertools\n",
    "from numbers import Number\n",
    "\n",
    "# local libraries\n",
    "from estimator import Estimator\n",
    "from util_classes import Dataset\n",
    "from optimizer import Optimizer\n",
    "from nn import NeuralNetwork, LinearLayer, ActivationFunction\n",
    "\n",
    "from loss import LossFunction\n",
    "\n",
    "# def filter_dict_by_key(dictionary: dict, keys: list[str]) -> dict:\n",
    "#     filtered_dict = {}\n",
    "#     for key in keys:\n",
    "#         if key in dictionary.keys():\n",
    "#             filtered_dict[key] = dictionary[key]\n",
    "#     return filtered_dict\n",
    "\n",
    "\n",
    "class GridSearch:\n",
    "    _net_keys = [\"layers\"]\n",
    "    _optimizer_keys = [\"eta\", \"l2_coeff\", \"alpha\"]\n",
    "    _loss_keys = [\"fname\"]\n",
    "    _estimator_keys = [\"batchsize\"]\n",
    "    # _global_keys = _net_keys + _optimizer_keys + _loss_keys + _estimator_keys\n",
    "\n",
    "    # dictionary containing translations from exposed names to names to pass to functions internally\n",
    "    _param_name_translations = {\n",
    "        \"layers\": \"layers\",\n",
    "        \"l2\": \"l2_coeff\",\n",
    "        \"momentum\": \"alpha\",\n",
    "        \"eta\": \"eta\",\n",
    "        \"loss\": \"fname\",\n",
    "        \"batchsize\": \"batchsize\",\n",
    "    }\n",
    "\n",
    "    # check param_grid validity\n",
    "    @staticmethod\n",
    "    def _check_param_grid(param_dict: dict, hyper_grid: dict) -> None:\n",
    "        \"\"\"Checks that the given grid of hyperparameters is correct\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        param_dict : dict\n",
    "            dictionary of accepted hyperparameters\n",
    "        hyper_grid : dict\n",
    "            dictionary containing the grid of hyperparameters on which to perform the validity check\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            when an hyperparameter is missing or one of its possible values is not valid\n",
    "        TypeError\n",
    "            when an hyperparameter has a value with the wrong type or is not a list of values\n",
    "        \"\"\"\n",
    "\n",
    "        activation_functions = [\"ReLU\", 'logistic', 'tanh', \"linear\"]\n",
    "        loss_functions = [\"MSE\"]\n",
    "\n",
    "        for key in param_dict.keys():\n",
    "            if not key in hyper_grid.keys():\n",
    "                raise ValueError(\n",
    "                    (\n",
    "                        \"All the following parameters must be present in the\"\n",
    "                        \" hyperparameter grid: \"\n",
    "                    ),\n",
    "                    list(param_dict.keys()),\n",
    "                )\n",
    "            if not isinstance(hyper_grid[key], list) or len(hyper_grid[key]) == 0:\n",
    "                raise ValueError(\n",
    "                    \"Each parameter must have an associated not empty list of\"\n",
    "                    \" parameters\"\n",
    "                )\n",
    "\n",
    "        # check layers\n",
    "        for net in hyper_grid[\"layers\"]:\n",
    "            for layer in net:\n",
    "                if (\n",
    "                    not isinstance(layer, tuple)\n",
    "                    or not len(layer) == 2\n",
    "                    or not isinstance(layer[0], int)\n",
    "                    or not isinstance(layer[1], str)\n",
    "                ):\n",
    "                    raise TypeError(\n",
    "                        \"The layers parameter accepts a list of tuples of length 2 with\"\n",
    "                        \" the first element being an integer that is the number of\"\n",
    "                        \" units in that layer, and the second element is a string\"\n",
    "                        \" containing the name of the activation function for that layer\"\n",
    "                    )\n",
    "                if layer[0] <= 0:\n",
    "                    raise ValueError(\"The number of units must be greater than 0\")\n",
    "                if not layer[1] in activation_functions:\n",
    "                    raise ValueError(\n",
    "                        (\n",
    "                            \"Only the following values are accepted for activation\"\n",
    "                            \" function: \"\n",
    "                        ),\n",
    "                        activation_functions,\n",
    "                    )\n",
    "\n",
    "        # check l2\n",
    "        for l2_coeff in hyper_grid[\"l2\"]:\n",
    "            if not isinstance(l2_coeff, Number):\n",
    "                raise TypeError(\"The L2 coefficient must be a number\")\n",
    "            if l2_coeff < 0:\n",
    "                raise ValueError(\"The L2 coefficient must be at least 0\")\n",
    "\n",
    "        # check momentum\n",
    "        for momentum in hyper_grid[\"momentum\"]:\n",
    "            if not isinstance(momentum, Number):\n",
    "                raise TypeError(\"The momentum parameter must be a number\")\n",
    "            if momentum < 0:\n",
    "                raise ValueError(\"The momentum parameter must be at least 0\")\n",
    "\n",
    "        # check eta\n",
    "        for eta in hyper_grid[\"eta\"]:\n",
    "            if not isinstance(eta, Number):\n",
    "                raise TypeError(\"The learning rate must be a number\")\n",
    "            if eta <= 0:\n",
    "                raise ValueError(\"The learning rate must be greater than 0\")\n",
    "\n",
    "        # check loss functions\n",
    "        for loss in hyper_grid[\"loss\"]:\n",
    "            if not isinstance(loss, str):\n",
    "                raise TypeError(\n",
    "                    \"The loss must be a string corresponding to the required loss\"\n",
    "                )\n",
    "            if loss not in loss_functions:\n",
    "                raise ValueError(\n",
    "                    \"Only the following values are accepted for the loss: \",\n",
    "                    loss_functions,\n",
    "                )\n",
    "\n",
    "        # check batchsize\n",
    "        for batchsize in hyper_grid[\"batchsize\"]:\n",
    "            if not isinstance(batchsize, int):\n",
    "                raise TypeError(\"The batch size must be an integer\")\n",
    "            if batchsize <= 0 and batchsize != -1:\n",
    "                raise ValueError(\n",
    "                    \"The batch size must be greater than 0. If -1 is passed as a value\"\n",
    "                    \" the size of the dataset will be used\"\n",
    "                )\n",
    "\n",
    "    def __init__(self, estimator: Estimator, hyper_grid: dict):\n",
    "        \"\"\"Initializes a new instance\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimator : Estimator\n",
    "            the estimator to use for training and evaluation\n",
    "        hyper_grid : dict\n",
    "            grid of hyperparameters\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            when parameter types are incorrect\n",
    "        \"\"\"\n",
    "        if estimator == None or type(estimator) != Estimator:\n",
    "            raise TypeError\n",
    "        self._estimator = estimator\n",
    "        if hyper_grid == None or type(hyper_grid) != dict:\n",
    "            raise TypeError\n",
    "\n",
    "        # check for wrong values\n",
    "        GridSearch._check_param_grid(self._param_name_translations, hyper_grid)\n",
    "\n",
    "        # translate names of parameters and sort by key for better efficiency\n",
    "        new_grid = {}\n",
    "        for key in self._param_name_translations.keys():\n",
    "            if key in hyper_grid:\n",
    "                new_grid[self._param_name_translations[key]] = hyper_grid[key]\n",
    "        self._hyper_grid = new_grid\n",
    "\n",
    "    # returns a list of data folds through indexes\n",
    "    def _generate_folds(\n",
    "        self, dataset: Dataset, n_folds: int\n",
    "    ) -> list[tuple[Dataset, Dataset]]:\n",
    "        \"\"\"function to generate the folds to use during grid search\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : Dataset\n",
    "            dataset to split in folds\n",
    "        n_folds : int\n",
    "            number of folds\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list(tuple[Dataset, Dataset])\n",
    "            returns a list containing tuples of Dataset classes. Each tuple is of the form (Training set, Test set)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        data_size = dataset.ids.shape[0]\n",
    "        indices = np.arange(data_size)\n",
    "\n",
    "        # TODO maybe shuffle not needed if we assume dataset has already been shuffled\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        folds = []\n",
    "\n",
    "        for index_lists in np.array_split(indices, n_folds):\n",
    "            # make mask to split test and training set indices\n",
    "            mask = np.zeros(data_size, dtype=bool)\n",
    "            mask[index_lists] = True\n",
    "            test_indices = indices[mask]\n",
    "            train_indices = indices[~mask]\n",
    "            # initialize test set and training set\n",
    "            test_set = Dataset(\n",
    "                ids=dataset.ids[test_indices],\n",
    "                labels=dataset.labels[test_indices],\n",
    "                data=dataset.data[test_indices],\n",
    "            )\n",
    "            train_set = Dataset(\n",
    "                ids=dataset.ids[train_indices],\n",
    "                labels=dataset.labels[train_indices],\n",
    "                data=dataset.data[train_indices],\n",
    "            )\n",
    "            folds.append((train_set, test_set))\n",
    "        return folds\n",
    "\n",
    "    def _create_estimator_params(self, combination: dict, input_dim: int) -> dict:\n",
    "        \"\"\"function to create the dictionary to pass to estimator for update\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        combination : dict\n",
    "            combination of hyperparameters to use\n",
    "        input_dim : int\n",
    "            number of features of dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            a dictionary to pass to the estimator's update function\n",
    "        \"\"\"\n",
    "        # filter parameters for various classes\n",
    "        loss_params = {key: combination[key] for key in self._loss_keys}\n",
    "        estimator_params = {key: combination[key] for key in self._estimator_keys}\n",
    "        optimizer_params = {key: combination[key] for key in self._optimizer_keys}\n",
    "        net_params = {key: combination[key] for key in self._net_keys}\n",
    "\n",
    "        # create dictionary of params to pass to constructors\n",
    "        estimator_params[\"loss\"] = LossFunction(**loss_params)\n",
    "        estimator_params[\"optimizer\"] = Optimizer(**optimizer_params)\n",
    "\n",
    "        # create list of layers to create NN\n",
    "        old_units = input_dim\n",
    "        layer_list = []\n",
    "        for layer in net_params[\"layers\"][:-1]:\n",
    "            layer_list.append(LinearLayer((old_units, layer[0])))\n",
    "            # TODO maybe linear layers can be removed\n",
    "            layer_list.append(ActivationFunction(fname=layer[1]))\n",
    "            old_units = layer[0]\n",
    "        last_layer = net_params[\"layers\"][-1]\n",
    "        layer_list.append(LinearLayer((old_units, last_layer[0])))\n",
    "        if last_layer[1] != \"linear\":\n",
    "            layer_list.append(ActivationFunction(fname=last_layer[1]))\n",
    "        estimator_params[\"net\"] = NeuralNetwork(layer_list)\n",
    "        return estimator_params\n",
    "\n",
    "    # returns the best set of hyperparameters\n",
    "    def k_fold(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        n_folds: int,\n",
    "        n_epochs: int,\n",
    "        callback: Callable[[dict], None] = print,\n",
    "        loss_list: list[str] = ['MSE']\n",
    "    ) -> list:\n",
    "        \"\"\"function to execute a k-fold cross-validation on the given dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : Dataset\n",
    "            dataset to use for k-fold cross-validation\n",
    "        n_folds : int\n",
    "            number of folds to use in cross-validation\n",
    "        n_epochs : int\n",
    "            number of epochs to run training\n",
    "        callback : Callable[[dict], None], optional\n",
    "            callback function to use during training, by default print\n",
    "        loss_list: list[str]\n",
    "            list of loss to functions to apply to test set\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            list containing results of the cross-validation ordered by increasing average loss on the test set.\n",
    "            Every element is a list of dictionaries containing the\n",
    "            combination of hyperparameters, the average of the test loss and the standard deviation on the test loss\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            when values of some parameters are incorrect\n",
    "        \"\"\"\n",
    "\n",
    "        hyper_grid = self._hyper_grid\n",
    "        estimator = self._estimator\n",
    "\n",
    "        data_size = dataset.shape[0]\n",
    "        input_dim = dataset.shape[1][0]\n",
    "        output_dim = dataset.shape[1][1]\n",
    "\n",
    "        # check n_folds value\n",
    "        if n_folds > data_size:\n",
    "            raise ValueError(\n",
    "                \"The number of folds cannot be greater than the number of samples in\"\n",
    "                \" the dataset\"\n",
    "            )\n",
    "        # check if output layer is correct for all combinations\n",
    "        for layers in hyper_grid[\"layers\"]:\n",
    "            if layers[-1][0] != output_dim:\n",
    "                raise ValueError(\n",
    "                    \"Number of units in last layer must be equal to the output\"\n",
    "                    \" dimension of the data\"\n",
    "                )\n",
    "        # check values for batchsize\n",
    "        for batchsize in hyper_grid[\"batchsize\"]:\n",
    "            if batchsize > data_size:\n",
    "                raise ValueError(\n",
    "                    \"The batchsize cannot be greater than the number of samples\"\n",
    "                )\n",
    "\n",
    "        folds = self._generate_folds(dataset=dataset, n_folds=n_folds)\n",
    "\n",
    "        # generates all combinations of hyperparameters\n",
    "        keys, values = zip(*hyper_grid.items())\n",
    "        param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "        combination_loss_list = []\n",
    "\n",
    "        # iterates all combinations of hyperparameters\n",
    "        for combination in param_combinations:\n",
    "\n",
    "            estimator_params = self._create_estimator_params(combination, input_dim)\n",
    "            if estimator_params[\"batchsize\"] == -1:\n",
    "                estimator_params[\"batchsize\"] = data_size\n",
    "            estimator.update_params(**estimator_params)\n",
    "\n",
    "            test_loss_list = []\n",
    "\n",
    "            print(combination)\n",
    "\n",
    "            # iterates folds of dataset\n",
    "            for train_set, test_set in folds:\n",
    "                estimator.train(dataset=train_set, n_epochs=n_epochs, callback=callback)\n",
    "                test_loss_list.append(estimator.evaluate(losses = loss_list, dataset = test_set))\n",
    "                estimator.reset()\n",
    "            test_loss_avg = {}\n",
    "            test_loss_std = {}\n",
    "            for loss in loss_list:\n",
    "                test_loss_avg[loss] = sum(d[loss] for d in test_loss_list) / len(test_loss_list)\n",
    "                test_loss_std[loss] = np.std([d[loss] for d in test_loss_list])\n",
    "\n",
    "            # test_loss_avg = sum(test_loss_list) / n_folds\n",
    "            # test_loss_std = np.std(test_loss_list)\n",
    "\n",
    "            combination_loss_list.append(\n",
    "                {\n",
    "                    \"parameters\": combination,\n",
    "                    \"test_loss_avg\": test_loss_avg,\n",
    "                    \"test_loss_std\": test_loss_std,\n",
    "                }\n",
    "            )\n",
    "            print(combination_loss_list)\n",
    "        if(loss_list[0] == 'binary_accuracy'):\n",
    "            combination_loss_list.sort(key=lambda x: x[\"test_loss_avg\"][loss_list[0]], reverse = True)\n",
    "        else:\n",
    "            combination_loss_list.sort(key=lambda x: x[\"test_loss_avg\"][loss_list[0]])\n",
    "        return combination_loss_list\n",
    "\n",
    "    # returns an estimation of the risk for the model, average +- standard deviation\n",
    "    def nested_k_fold(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        inner_n_folds: int,\n",
    "        outer_n_folds: int,\n",
    "        n_epochs: int,\n",
    "        inner_callback: Callable[[dict], None] = print,\n",
    "        outer_callback: Callable[[dict], None] = print,\n",
    "    ) -> dict:\n",
    "        \"\"\"function implementing nested k-fold cross validation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : Dataset\n",
    "            dataset to run cross validation on\n",
    "        inner_n_folds : int\n",
    "            number of folds for the inner cross validation\n",
    "        outer_n_folds : int\n",
    "            number of folds for the outer cross validation\n",
    "        n_epochs : int\n",
    "            number of epochs to run training for\n",
    "        inner_callback : Callable[[dict], None], optional\n",
    "            callback function to use during training for the inner cross validation, by default print\n",
    "        outer_callback : Callable[[dict], None], optional\n",
    "            callback function to use during training for the outer cross validation, by default print\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            a dictionary containing a list of tuples each containing the best combination of hyperparameters\n",
    "            on that fold and the corresponding loss on the test set for that fold, the average loss on the test sets across the folds\n",
    "            and their standard deviation\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            when values are incorrect\n",
    "        \"\"\"\n",
    "\n",
    "        estimator = self._estimator\n",
    "        data_size = dataset.shape[0]\n",
    "        folds = self._generate_folds(dataset=dataset, n_folds=outer_n_folds)\n",
    "        input_dim = dataset.shape[1][0]\n",
    "\n",
    "        # check outer_n_folds value\n",
    "        if outer_n_folds > data_size:\n",
    "            raise ValueError(\n",
    "                \"The number of folds cannot be greater than the number of samples in\"\n",
    "                \" the dataset\"\n",
    "            )\n",
    "\n",
    "        # check inner_n_folds value\n",
    "        if inner_n_folds > data_size:\n",
    "            raise ValueError(\n",
    "                \"The number of folds cannot be greater than the number of samples in\"\n",
    "                \" the dataset\"\n",
    "            )\n",
    "\n",
    "        test_loss_list = []\n",
    "        param_combination_list = []\n",
    "\n",
    "        for train_set, test_set in folds:\n",
    "            train_results = self.k_fold(\n",
    "                dataset=train_set,\n",
    "                n_folds=inner_n_folds,\n",
    "                n_epochs=n_epochs,\n",
    "                callback=inner_callback,\n",
    "            )\n",
    "            params = train_results[0][\"parameters\"]\n",
    "            estimator_params = self._create_estimator_params(params, input_dim)\n",
    "            if estimator_params[\"batchsize\"] == -1:\n",
    "                estimator_params[\"batchsize\"] = data_size\n",
    "\n",
    "            estimator.update_params(**estimator_params)\n",
    "            estimator.train(\n",
    "                dataset=train_set, n_epochs=n_epochs, callback=outer_callback\n",
    "            )\n",
    "            test_loss_list.append(estimator.evaluate(test_set))\n",
    "            param_combination_list.append(params)\n",
    "\n",
    "        test_loss_avg = sum(test_loss_list) / outer_n_folds\n",
    "        test_loss_std = np.std(test_loss_list)\n",
    "\n",
    "        results = {\n",
    "            \"test_loss_list\": list(zip(param_combination_list, test_loss_list)),\n",
    "            \"test_loss_avg\": test_loss_avg,\n",
    "            \"test_loss_std\": test_loss_std,\n",
    "        }\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork(\n",
    "    [\n",
    "        LinearLayer((8, 16)),\n",
    "        ActivationFunction(),\n",
    "        LinearLayer((16, 16)),\n",
    "        ActivationFunction(),\n",
    "        LinearLayer((16, 2)),\n",
    "    ]\n",
    ")\n",
    "estimator = Estimator(net)\n",
    "grid = {}\n",
    "grid[\"eta\"] = [10e-3]\n",
    "grid[\"momentum\"] = [0.2]\n",
    "grid[\"layers\"] = [[(8, \"ReLU\"), (1, \"linear\")], [(4, \"ReLU\"), (1, \"linear\")]]\n",
    "grid[\"loss\"] = [\"MSE\"]\n",
    "grid[\"l2\"] = [10e-2]\n",
    "grid[\"batchsize\"] = [-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import read_monks, read_ML_cup\n",
    "\n",
    "# data = read_ML_cup(\"train\")\n",
    "data = read_monks(1, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layers': [(8, 'ReLU'), (1, 'linear')], 'l2_coeff': 0.1, 'alpha': 0.2, 'eta': 0.01, 'fname': 'MSE', 'batchsize': -1}\n",
      "{'epoch': 1, 'loss': 533.6682410725465}\n",
      "{'epoch': 2, 'loss': 0.2089354648232199}\n",
      "{'epoch': 3, 'loss': 0.25254477147778764}\n",
      "{'epoch': 4, 'loss': 0.25760474397842303}\n",
      "{'epoch': 5, 'loss': 0.2528866073285979}\n",
      "{'epoch': 6, 'loss': 0.24731637364048095}\n",
      "{'epoch': 7, 'loss': 0.2428197579235229}\n",
      "{'epoch': 8, 'loss': 0.2395144762576019}\n",
      "[[0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]\n",
      " [0.01099011]] [1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0]\n",
      "{'epoch': 1, 'loss': 208.8883950141158}\n",
      "{'epoch': 2, 'loss': 0.1858129763458394}\n",
      "{'epoch': 3, 'loss': 0.22373846607185963}\n",
      "{'epoch': 4, 'loss': 0.2396409650264344}\n",
      "{'epoch': 5, 'loss': 0.24693878370340108}\n",
      "{'epoch': 6, 'loss': 0.2509051104157132}\n",
      "{'epoch': 7, 'loss': 0.25336918469797015}\n",
      "{'epoch': 8, 'loss': 0.25501828004242333}\n",
      "[[0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]\n",
      " [0.03126925]] [0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1]\n",
      "{'epoch': 1, 'loss': 444.0075128794348}\n",
      "{'epoch': 2, 'loss': 0.13555045094169343}\n",
      "{'epoch': 3, 'loss': 0.17225272738638875}\n",
      "{'epoch': 4, 'loss': 0.19579732512416081}\n",
      "{'epoch': 5, 'loss': 0.2098920518738989}\n",
      "{'epoch': 6, 'loss': 0.21903683250300135}\n",
      "{'epoch': 7, 'loss': 0.2253218431804451}\n",
      "{'epoch': 8, 'loss': 0.2297551614026705}\n",
      "[[0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]\n",
      " [0.0404593]] [1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0]\n",
      "{'epoch': 1, 'loss': 544.8200691143992}\n",
      "{'epoch': 2, 'loss': 0.3475821858147325}\n",
      "{'epoch': 3, 'loss': 0.3693631710759966}\n",
      "{'epoch': 4, 'loss': 0.3412466454460624}\n",
      "{'epoch': 5, 'loss': 0.3111918020776474}\n",
      "{'epoch': 6, 'loss': 0.2883553052175304}\n",
      "{'epoch': 7, 'loss': 0.2723202717114901}\n",
      "{'epoch': 8, 'loss': 0.2612665322233182}\n",
      "[[-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]\n",
      " [-0.01232481]] [0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1]\n",
      "{'epoch': 1, 'loss': 408.9170408681275}\n",
      "{'epoch': 2, 'loss': 0.14640093170604535}\n",
      "{'epoch': 3, 'loss': 0.1830529221324877}\n",
      "{'epoch': 4, 'loss': 0.20092427009579944}\n",
      "{'epoch': 5, 'loss': 0.20963226552128233}\n",
      "{'epoch': 6, 'loss': 0.21453080432538868}\n",
      "{'epoch': 7, 'loss': 0.21763517641613725}\n",
      "{'epoch': 8, 'loss': 0.21973587625753502}\n",
      "[[0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]\n",
      " [0.03034197]] [0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1]\n",
      "[{'parameters': {'layers': [(8, 'ReLU'), (1, 'linear')], 'l2_coeff': 0.1, 'alpha': 0.2, 'eta': 0.01, 'fname': 'MSE', 'batchsize': -1}, 'test_loss_avg': {'binary_accuracy': 0.499}, 'test_loss_std': {'binary_accuracy': 0.10239140588936164}}]\n",
      "{'layers': [(4, 'ReLU'), (1, 'linear')], 'l2_coeff': 0.1, 'alpha': 0.2, 'eta': 0.01, 'fname': 'MSE', 'batchsize': -1}\n",
      "{'epoch': 1, 'loss': 92.38018998539442}\n",
      "{'epoch': 2, 'loss': 0.17522793068301382}\n",
      "{'epoch': 3, 'loss': 0.14214294435887798}\n",
      "{'epoch': 4, 'loss': 0.161870124290915}\n",
      "{'epoch': 5, 'loss': 0.17902783099757086}\n",
      "{'epoch': 6, 'loss': 0.1927266119012143}\n",
      "{'epoch': 7, 'loss': 0.20322361126532}\n",
      "{'epoch': 8, 'loss': 0.2110663524569179}\n",
      "[[0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]\n",
      " [0.05602992]] [1 0 1 1 1 1 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1 0 1 0]\n",
      "{'epoch': 1, 'loss': 94.08782211939508}\n",
      "{'epoch': 2, 'loss': 0.34406434879285963}\n",
      "{'epoch': 3, 'loss': 0.33167111170257363}\n",
      "{'epoch': 4, 'loss': 0.3155948116400968}\n",
      "{'epoch': 5, 'loss': 0.29983432268283133}\n",
      "{'epoch': 6, 'loss': 0.2877976610403795}\n",
      "{'epoch': 7, 'loss': 0.279185848195662}\n",
      "{'epoch': 8, 'loss': 0.273137024884572}\n",
      "[[0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]\n",
      " [0.00705434]] [0 0 1 0 1 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1]\n",
      "{'epoch': 1, 'loss': 44.243077068635095}\n",
      "{'epoch': 2, 'loss': 1.5150467353565535}\n",
      "{'epoch': 3, 'loss': 0.16678283010813935}\n",
      "{'epoch': 4, 'loss': 0.14037518068023638}\n",
      "{'epoch': 5, 'loss': 0.158417539637432}\n",
      "{'epoch': 6, 'loss': 0.1773711518636518}\n",
      "{'epoch': 7, 'loss': 0.19325902216303315}\n",
      "{'epoch': 8, 'loss': 0.20580102820592375}\n",
      "[[0.08011926]\n",
      " [0.07894813]\n",
      " [0.07914622]\n",
      " [0.08011926]\n",
      " [0.08011926]\n",
      " [0.08011926]\n",
      " [0.08011926]\n",
      " [0.08011926]\n",
      " [0.08011926]\n",
      " [0.08011926]\n",
      " [0.08011926]\n",
      " [0.08011926]\n",
      " [0.08011926]\n",
      " [0.079427  ]\n",
      " [0.08011926]\n",
      " [0.0754684 ]\n",
      " [0.08011926]\n",
      " [0.08011926]\n",
      " [0.07968425]\n",
      " [0.08011926]\n",
      " [0.07932283]\n",
      " [0.08011926]\n",
      " [0.07626368]\n",
      " [0.07668396]\n",
      " [0.07997861]] [1 0 0 0 0 1 1 1 1 1 0 1 1 1 0 0 0 1 0 1 0 1 0 0 0]\n",
      "{'epoch': 1, 'loss': 23.024062707557423}\n",
      "{'epoch': 2, 'loss': 1.09079232231088}\n",
      "{'epoch': 3, 'loss': 0.6066621884522909}\n",
      "{'epoch': 4, 'loss': 0.30211621983168646}\n",
      "{'epoch': 5, 'loss': 0.23488572716240916}\n",
      "{'epoch': 6, 'loss': 0.2193302116476708}\n",
      "{'epoch': 7, 'loss': 0.21752714781256366}\n",
      "{'epoch': 8, 'loss': 0.2198107404104703}\n",
      "[[0.0530656 ]\n",
      " [0.05075248]\n",
      " [0.05338409]\n",
      " [0.05312615]\n",
      " [0.05084259]\n",
      " [0.05196417]\n",
      " [0.04894032]\n",
      " [0.05268792]\n",
      " [0.05367858]\n",
      " [0.05416114]\n",
      " [0.05117624]\n",
      " [0.05241637]\n",
      " [0.04901409]\n",
      " [0.05244672]\n",
      " [0.05681959]\n",
      " [0.05378719]\n",
      " [0.05461118]\n",
      " [0.05350383]\n",
      " [0.04768751]\n",
      " [0.05454075]\n",
      " [0.05645632]\n",
      " [0.05589029]\n",
      " [0.05189988]\n",
      " [0.04925288]\n",
      " [0.05225946]] [0 1 0 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1]\n",
      "{'epoch': 1, 'loss': 128.79997923723693}\n",
      "{'epoch': 2, 'loss': 0.25397992397819613}\n",
      "{'epoch': 3, 'loss': 0.26724028040830056}\n",
      "{'epoch': 4, 'loss': 0.26032863781903925}\n",
      "{'epoch': 5, 'loss': 0.25108388539459553}\n",
      "{'epoch': 6, 'loss': 0.24352711136165084}\n",
      "{'epoch': 7, 'loss': 0.23798836055126432}\n",
      "{'epoch': 8, 'loss': 0.23405803023364136}\n",
      "[[0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]\n",
      " [0.00795155]] [0 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 0 1 1 1]\n",
      "[{'parameters': {'layers': [(8, 'ReLU'), (1, 'linear')], 'l2_coeff': 0.1, 'alpha': 0.2, 'eta': 0.01, 'fname': 'MSE', 'batchsize': -1}, 'test_loss_avg': {'binary_accuracy': 0.499}, 'test_loss_std': {'binary_accuracy': 0.10239140588936164}}, {'parameters': {'layers': [(4, 'ReLU'), (1, 'linear')], 'l2_coeff': 0.1, 'alpha': 0.2, 'eta': 0.01, 'fname': 'MSE', 'batchsize': -1}, 'test_loss_avg': {'binary_accuracy': 0.499}, 'test_loss_std': {'binary_accuracy': 0.10239140588936164}}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'parameters': {'layers': [(8, 'ReLU'), (1, 'linear')],\n",
       "   'l2_coeff': 0.1,\n",
       "   'alpha': 0.2,\n",
       "   'eta': 0.01,\n",
       "   'fname': 'MSE',\n",
       "   'batchsize': -1},\n",
       "  'test_loss_avg': {'binary_accuracy': 0.499},\n",
       "  'test_loss_std': {'binary_accuracy': 0.10239140588936164}},\n",
       " {'parameters': {'layers': [(4, 'ReLU'), (1, 'linear')],\n",
       "   'l2_coeff': 0.1,\n",
       "   'alpha': 0.2,\n",
       "   'eta': 0.01,\n",
       "   'fname': 'MSE',\n",
       "   'batchsize': -1},\n",
       "  'test_loss_avg': {'binary_accuracy': 0.499},\n",
       "  'test_loss_std': {'binary_accuracy': 0.10239140588936164}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = GridSearch(estimator, grid)\n",
    "selector.k_fold(data, 5, n_epochs=8, loss_list=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'layers': [(8, 'ReLU'), (1, 'tanh')], 'l2_coeff': 0.1, 'alpha': 0.2, 'eta': 0.01, 'fname': 'MSE', 'batchsize': -1}\n",
      "{'epoch': 1, 'loss': 0.26582278481007754}\n",
      "{'epoch': 2, 'loss': 0.26582278382269425}\n",
      "{'epoch': 3, 'loss': 0.26581924099146403}\n",
      "{'epoch': 4, 'loss': 0.2650335010837216}\n",
      "{'epoch': 5, 'loss': 0.24710167857476026}\n",
      "{'epoch': 1, 'loss': 0.23417721518987342}\n",
      "{'epoch': 2, 'loss': 0.2341772151763254}\n",
      "{'epoch': 3, 'loss': 0.234176894596665}\n",
      "{'epoch': 4, 'loss': 0.2339988574694747}\n",
      "{'epoch': 5, 'loss': 0.2267940311848102}\n",
      "{'epoch': 1, 'loss': 0.25316455696202533}\n",
      "{'epoch': 2, 'loss': 0.25316455696062107}\n",
      "{'epoch': 3, 'loss': 0.2531644413394418}\n",
      "{'epoch': 4, 'loss': 0.2530513578439216}\n",
      "{'epoch': 5, 'loss': 0.24715413665359956}\n",
      "{'epoch': 1, 'loss': 0.22151898734177203}\n",
      "{'epoch': 2, 'loss': 0.2215189872888348}\n",
      "{'epoch': 3, 'loss': 0.22151831881371536}\n",
      "{'epoch': 4, 'loss': 0.22125811410805812}\n",
      "{'epoch': 5, 'loss': 0.2129357656738729}\n",
      "{'epoch': 1, 'loss': 0.23749999999999197}\n",
      "{'epoch': 2, 'loss': 0.2374999991048205}\n",
      "{'epoch': 3, 'loss': 0.23749543166535717}\n",
      "{'epoch': 4, 'loss': 0.23657949480976911}\n",
      "{'epoch': 5, 'loss': 0.21881928247925247}\n",
      "[{'parameters': {'layers': [(8, 'ReLU'), (1, 'tanh')], 'l2_coeff': 0.1, 'alpha': 0.2, 'eta': 0.01, 'fname': 'MSE', 'batchsize': -1}, 'test_loss_avg': {'MSE': 0.18091298487831337}, 'test_loss_std': {'MSE': 0.04757070673622821}}]\n",
      "{'layers': [(4, 'ReLU'), (1, 'tanh')], 'l2_coeff': 0.1, 'alpha': 0.2, 'eta': 0.01, 'fname': 'MSE', 'batchsize': -1}\n",
      "{'epoch': 1, 'loss': 0.26582278478521754}\n",
      "{'epoch': 2, 'loss': 0.26582273026965186}\n",
      "{'epoch': 3, 'loss': 0.26578568505220507}\n",
      "{'epoch': 4, 'loss': 0.2629537383117511}\n",
      "{'epoch': 5, 'loss': 0.23127294976952004}\n",
      "{'epoch': 1, 'loss': 0.23417721385474977}\n",
      "{'epoch': 2, 'loss': 0.2341755139573861}\n",
      "{'epoch': 3, 'loss': 0.2338255737161622}\n",
      "{'epoch': 4, 'loss': 0.22439747971415316}\n",
      "{'epoch': 5, 'loss': 0.1776828263304398}\n",
      "{'epoch': 1, 'loss': 0.25316455696155366}\n",
      "{'epoch': 2, 'loss': 0.25316454747701833}\n",
      "{'epoch': 3, 'loss': 0.253148053752686}\n",
      "{'epoch': 4, 'loss': 0.2514199423317546}\n",
      "{'epoch': 5, 'loss': 0.22842052550208697}\n",
      "{'epoch': 1, 'loss': 0.2215189854405513}\n",
      "{'epoch': 2, 'loss': 0.22151712009398908}\n",
      "{'epoch': 3, 'loss': 0.22116411623209187}\n",
      "{'epoch': 4, 'loss': 0.2122481381160544}\n",
      "{'epoch': 5, 'loss': 0.17122008924706933}\n",
      "{'epoch': 1, 'loss': 0.23749999977561584}\n",
      "{'epoch': 2, 'loss': 0.23749957952905856}\n",
      "{'epoch': 3, 'loss': 0.23735934841901907}\n",
      "{'epoch': 4, 'loss': 0.2314791956025534}\n",
      "{'epoch': 5, 'loss': 0.19072723039759282}\n",
      "[{'parameters': {'layers': [(8, 'ReLU'), (1, 'tanh')], 'l2_coeff': 0.1, 'alpha': 0.2, 'eta': 0.01, 'fname': 'MSE', 'batchsize': -1}, 'test_loss_avg': {'MSE': 0.18091298487831337}, 'test_loss_std': {'MSE': 0.04757070673622821}}, {'parameters': {'layers': [(4, 'ReLU'), (1, 'tanh')], 'l2_coeff': 0.1, 'alpha': 0.2, 'eta': 0.01, 'fname': 'MSE', 'batchsize': -1}, 'test_loss_avg': {'MSE': 0.1357763639820512}, 'test_loss_std': {'MSE': 0.014708959002637825}}]\n",
      "{'epoch': 1, 'loss': 0.2424242397861007}\n",
      "{'epoch': 2, 'loss': 0.24242290746115505}\n",
      "{'epoch': 3, 'loss': 0.24216158805262492}\n",
      "{'epoch': 4, 'loss': 0.23447882391741484}\n",
      "{'epoch': 5, 'loss': 0.19201479836669655}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Estimator.evaluate() missing 1 required positional argument: 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m selector\u001b[39m.\u001b[39;49mnested_k_fold(data, \u001b[39m5\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39m5\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[1], line 462\u001b[0m, in \u001b[0;36mGridSearch.nested_k_fold\u001b[0;34m(self, dataset, inner_n_folds, outer_n_folds, n_epochs, inner_callback, outer_callback)\u001b[0m\n\u001b[1;32m    458\u001b[0m     estimator\u001b[39m.\u001b[39mupdate_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mestimator_params)\n\u001b[1;32m    459\u001b[0m     estimator\u001b[39m.\u001b[39mtrain(\n\u001b[1;32m    460\u001b[0m         dataset\u001b[39m=\u001b[39mtrain_set, n_epochs\u001b[39m=\u001b[39mn_epochs, callback\u001b[39m=\u001b[39mouter_callback\n\u001b[1;32m    461\u001b[0m     )\n\u001b[0;32m--> 462\u001b[0m     test_loss_list\u001b[39m.\u001b[39mappend(estimator\u001b[39m.\u001b[39;49mevaluate(test_set))\n\u001b[1;32m    463\u001b[0m     param_combination_list\u001b[39m.\u001b[39mappend(params)\n\u001b[1;32m    465\u001b[0m test_loss_avg \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(test_loss_list) \u001b[39m/\u001b[39m outer_n_folds\n",
      "\u001b[0;31mTypeError\u001b[0m: Estimator.evaluate() missing 1 required positional argument: 'dataset'"
     ]
    }
   ],
   "source": [
    "selector.nested_k_fold(data, 5, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = np.array([1,2,3,4,5])\n",
    "tmp.reshape(1,5).ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bd504499a3d325a7c1da9f8228712639636db49ae66a9009fa19a793144457f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
