{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external libraries\n",
    "import numpy as np\n",
    "from typing import Iterator, Callable\n",
    "import itertools\n",
    "\n",
    "# local libraries\n",
    "from estimator import Estimator\n",
    "from util_classes import Dataset\n",
    "from optimizer import Optimizer\n",
    "from nn import NeuralNetwork, LinearLayer, ActivationFunction\n",
    "\n",
    "from loss import LossFunction\n",
    "\n",
    "# def filter_dict_by_key(dictionary: dict, keys: list[str]) -> dict:\n",
    "#     filtered_dict = {}\n",
    "#     for key in keys:\n",
    "#         if key in dictionary.keys():\n",
    "#             filtered_dict[key] = dictionary[key]\n",
    "#     return filtered_dict\n",
    "\n",
    "\n",
    "class GridSearch:\n",
    "    _net_keys = [\"layers\"]\n",
    "    _optimizer_keys = [\"eta\", \"l2_coeff\", \"alpha\"]\n",
    "    _loss_keys = [\"fname\"]\n",
    "    _estimator_keys = [\"batchsize\"]\n",
    "    # _global_keys = _net_keys + _optimizer_keys + _loss_keys + _estimator_keys\n",
    "\n",
    "    # dictionary containing translations from exposed names to names to pass to functions internally\n",
    "    _param_name_translations = {\n",
    "        \"layers\": \"layers\",\n",
    "        \"l2\": \"l2_coeff\",\n",
    "        \"momentum\": \"alpha\",\n",
    "        \"eta\": \"eta\",\n",
    "        \"loss\": \"fname\",\n",
    "        \"batchsize\": \"batchsize\",\n",
    "    }\n",
    "\n",
    "    # check param_grid and remove invalid values\n",
    "    # TODO implement\n",
    "    @staticmethod\n",
    "    def _check_param_grid(hyper_grid) -> bool:\n",
    "        return True\n",
    "\n",
    "    def __init__(self, estimator: Estimator, hyper_grid: dict):\n",
    "        if estimator == None or type(estimator) != Estimator:\n",
    "            raise TypeError\n",
    "        self._estimator = estimator\n",
    "        if hyper_grid == None or type(hyper_grid) != dict:\n",
    "            raise TypeError\n",
    "\n",
    "        # check for wrong values\n",
    "        if not GridSearch._check_param_grid(hyper_grid):\n",
    "            raise ValueError\n",
    "\n",
    "        # translate names of parameters and sort by key for better efficiency\n",
    "        new_grid = {}\n",
    "        for key in self._param_name_translations.keys():\n",
    "            if key in hyper_grid:\n",
    "                new_grid[self._param_name_translations[key]] = hyper_grid[key]\n",
    "        self._hyper_grid = new_grid\n",
    "\n",
    "    # returns a list of data folds through indexes\n",
    "    def _generate_folds(self, dataset: Dataset, n_folds: int) -> Iterator[tuple[Dataset, Dataset]]:\n",
    "        data_size = dataset.ids.shape[0]\n",
    "        indices = np.arange(data_size)\n",
    "\n",
    "        # TODO maybe shuffle not needed if we assume dataset has already been shuffled\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        folds = []\n",
    "\n",
    "        for index_lists in np.array_split(indices, n_folds):\n",
    "            # make mask to split test and training set indices\n",
    "            mask = np.zeros(data_size, dtype=bool)\n",
    "            mask[index_lists] = True\n",
    "            test_indices = indices[mask]\n",
    "            train_indices = indices[~mask]\n",
    "            # initialize test set and training set\n",
    "            test_set = Dataset(\n",
    "                ids=dataset.ids[test_indices],\n",
    "                labels=dataset.labels[test_indices],\n",
    "                data=dataset.data[test_indices],\n",
    "            )\n",
    "            train_set = Dataset(\n",
    "                ids=dataset.ids[train_indices],\n",
    "                labels=dataset.labels[train_indices],\n",
    "                data=dataset.data[train_indices],\n",
    "            )\n",
    "            folds.append((train_set, test_set))\n",
    "        return folds\n",
    "\n",
    "    def _create_estimator_params(self, combination: dict, input_dim: int) -> dict:\n",
    "        # filter parameters for various classes\n",
    "        loss_params = {key: combination[key] for key in self._loss_keys}\n",
    "        estimator_params = {key: combination[key] for key in self._estimator_keys}\n",
    "        optimizer_params = {key: combination[key] for key in self._optimizer_keys}\n",
    "        net_params = {key: combination[key] for key in self._net_keys}\n",
    "\n",
    "        # create dictionary of params to pass to constructors\n",
    "        estimator_params[\"loss\"] = LossFunction(**loss_params)\n",
    "        estimator_params[\"optimizer\"] = Optimizer(**optimizer_params)\n",
    "\n",
    "        # create list of layers to create NN\n",
    "        old_units = input_dim\n",
    "        layer_list = []\n",
    "        for layer in net_params[\"layers\"][:-1]:\n",
    "            layer_list.append(LinearLayer((old_units, layer[0])))\n",
    "            # TODO maybe linear layers can be removed\n",
    "            layer_list.append(ActivationFunction(fname=layer[1]))\n",
    "            old_units = layer[0]\n",
    "        last_layer = net_params[\"layers\"][-1]\n",
    "        layer_list.append(LinearLayer((old_units, last_layer[0])))\n",
    "        if last_layer[1] != \"linear\":\n",
    "            layer_list.append(ActivationFunction(fname=layer_list[1]))\n",
    "        estimator_params[\"net\"] = NeuralNetwork(layer_list)\n",
    "        return estimator_params\n",
    "\n",
    "\n",
    "    # returns the best set of hyperparameters\n",
    "    def k_fold(self, dataset: Dataset, n_folds: int, n_epochs: int, callback: Callable[[dict], None] = print) -> dict:\n",
    "        data_size = dataset.ids.shape[0]\n",
    "        if n_folds > data_size:\n",
    "            raise ValueError\n",
    "\n",
    "        hyper_grid = self._hyper_grid\n",
    "        estimator = self._estimator\n",
    "        input_dim = dataset.shape[1][0]\n",
    "        output_dim = dataset.shape[1][1]\n",
    "\n",
    "        # check if output layer is correct for all combinations\n",
    "        for layers in hyper_grid[\"layers\"]:\n",
    "            if layers[-1][0] != output_dim:\n",
    "                raise ValueError(\n",
    "                    \"Number of units in last layer must be equal to the output dimension of the data\"\n",
    "                )\n",
    "\n",
    "        folds = self._generate_folds(dataset = dataset, n_folds = n_folds)\n",
    "\n",
    "        # generates all combinations of hyperparameters\n",
    "        keys, values = zip(*hyper_grid.items())\n",
    "        param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "        combination_loss_list = []\n",
    "\n",
    "        # iterates all combinations of hyperparameters\n",
    "        for combination in param_combinations:\n",
    "            \n",
    "            estimator_params = self._create_estimator_params(combination, input_dim)\n",
    "            estimator.update_params(**estimator_params)\n",
    "\n",
    "            test_loss_list = []\n",
    "\n",
    "            print(combination)\n",
    "\n",
    "            # iterates folds of dataset\n",
    "            for train_set, test_set in folds:\n",
    "                estimator.train(dataset=train_set, n_epochs=n_epochs, callback = callback)\n",
    "                test_loss_list.append(estimator.evaluate(test_set))\n",
    "                estimator.reset()\n",
    "\n",
    "            test_loss_avg = sum(test_loss_list) / n_folds\n",
    "            test_loss_std = np.std(test_loss_list)\n",
    "\n",
    "            combination_loss_list.append({'parameters': combination, 'test_loss_avg': test_loss_avg, 'test_loss_std': test_loss_std})\n",
    "\n",
    "        combination_loss_list.sort(key=lambda x: x['test_loss_avg'])\n",
    "        return combination_loss_list\n",
    "\n",
    "    # returns an estimation of the risk for the model, average +- standard deviation\n",
    "    def nested_k_fold(self, dataset: Dataset, inner_n_folds: int, outer_n_folds: int, n_epochs: int,\n",
    "                        inner_callback: Callable[[dict], None] = print, outer_callback: Callable[[dict], None] = print) -> dict:\n",
    "        estimator = self._estimator\n",
    "        folds = self._generate_folds(dataset = dataset, n_folds = outer_n_folds)\n",
    "        input_dim = dataset.shape[1][0]\n",
    "        test_loss_list = []\n",
    "        \n",
    "        for train_set, test_set in folds:\n",
    "            train_results = self.k_fold(dataset = train_set, n_folds = inner_n_folds, n_epochs = n_epochs, callback = inner_callback)\n",
    "            params = train_results[0]['parameters']\n",
    "            estimator_params = self._create_estimator_params(params, input_dim)\n",
    "            estimator.update_params(**estimator_params)\n",
    "            estimator.train(dataset = train_set, n_epochs = n_epochs, callback = outer_callback)\n",
    "            test_loss_list.append(estimator.evaluate(test_set))\n",
    "        \n",
    "        test_loss_avg = sum(test_loss_list) / outer_n_folds\n",
    "        test_loss_std = np.std(test_loss_list)\n",
    "\n",
    "        results = {'test_loss_list': test_loss_list, 'test_loss_avg': test_loss_avg, 'test_loss_std': test_loss_std}\n",
    "        return results\n",
    "        \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork(\n",
    "    [\n",
    "        LinearLayer((8, 16)),\n",
    "        ActivationFunction(),\n",
    "        LinearLayer((16, 16)),\n",
    "        ActivationFunction(),\n",
    "        LinearLayer((16, 2)),\n",
    "    ]\n",
    ")\n",
    "estimator = Estimator(net)\n",
    "grid = {}\n",
    "grid[\"eta\"] = [0.001, 0.002]\n",
    "grid[\"momentum\"] = [0.1, 0.2]\n",
    "grid[\"layers\"] = [[(8, \"ReLU\"), (1, \"linear\")], [(4, \"ReLU\"), (1, \"linear\")]]\n",
    "grid[\"loss\"] = [\"MSE\"]\n",
    "grid[\"l2\"] = [0.1, 0.2]\n",
    "grid[\"batchsize\"] = [124]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import read_monks, read_ML_cup\n",
    "\n",
    "# data = _ML_cup(\"train\")\n",
    "data = read_monks(1, \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = GridSearch(estimator, grid)\n",
    "selector.k_fold(data, 5, n_epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.nested_k_fold(data, 5, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bd504499a3d325a7c1da9f8228712639636db49ae66a9009fa19a793144457f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
