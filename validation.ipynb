{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# external libraries\n",
    "import numpy as np\n",
    "from typing import Iterator\n",
    "import itertools\n",
    "# local libraries\n",
    "from estimator import Estimator\n",
    "from util_classes import Dataset\n",
    "from nn import NeuralNetwork, LinearLayer, ActivationFunction\n",
    "\n",
    "from loss import LossFunction\n",
    "\n",
    "def filter_dict_by_key(dictionary: dict, keys: list[str]) -> dict:\n",
    "    filtered_dict = {}\n",
    "    for key in keys:\n",
    "        if key in dictionary.keys():\n",
    "            filtered_dict[key] = dictionary[key]\n",
    "    return filtered_dict\n",
    "\n",
    "class GridSearch():\n",
    "    _net_keys = ['layers']\n",
    "    _optimizer_keys = ['eta', 'l2', 'momentum']\n",
    "    _loss_keys = ['loss']\n",
    "    _estimator_keys = ['batchsize', 'seed']\n",
    "    _global_keys = _net_keys + _optimizer_keys + _loss_keys + _estimator_keys\n",
    "\n",
    "    #dictionary containing translations from exposed names to names to pass to functions internally\n",
    "    _param_name_translations = {'l2': 'l2_coeff', 'momentum': 'alpha', 'loss': 'fname'}\n",
    "\n",
    "    #check param_grid and remove invalid values\n",
    "    #TODO implement\n",
    "    @staticmethod\n",
    "    def _check_param_grid(hyper_grid) -> bool:\n",
    "        return True\n",
    "\n",
    "    def __init__(self, estimator: Estimator, hyper_grid: dict):\n",
    "        if estimator == None or type(estimator) != Estimator:\n",
    "            raise TypeError\n",
    "        self._estimator = estimator\n",
    "        if hyper_grid == None or type(hyper_grid) != dict:\n",
    "            raise TypeError\n",
    "\n",
    "        #check for wrong values\n",
    "        if not GridSearch._check_param_grid(hyper_grid):\n",
    "            raise ValueError\n",
    "\n",
    "        #filter only accepted inputs and sort for better efficiency\n",
    "        #TODO maybe put in separate function\n",
    "        new_grid = {}\n",
    "        for key in self._global_keys:\n",
    "            if key in hyper_grid:\n",
    "                if key in self._param_name_translations:\n",
    "                    new_grid[self._param_name_translations[key]] = hyper_grid.pop(key)\n",
    "                else:\n",
    "                    new_grid[key] = hyper_grid.pop(key)\n",
    "        if hyper_grid:\n",
    "            rejected_params = list(hyper_grid.keys())\n",
    "            print('The following parameters were not accepted: ', rejected_params, '.\\nOnly the following parameters are accepted',\n",
    "            self._global_keys, '.\\n')\n",
    "        self._hyper_grid = new_grid\n",
    "\n",
    "    #returns a list of data folds through indexes\n",
    "    def _generate_folds(self) -> Iterator[tuple[Dataset, Dataset]]:\n",
    "        n_folds = self._n_folds\n",
    "        dataset = self._dataset\n",
    "        data_size = dataset.ids.shape[0]\n",
    "        indices = np.arange(data_size)\n",
    "        \n",
    "        #TODO maybe shuffle not needed if we assume dataset has already been shuffled\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for index_lists in np.array_split(indices, n_folds):\n",
    "            #make mask to split test and training set indices\n",
    "            mask = np.zeros(data_size, dtype=bool)\n",
    "            mask[index_lists] = True\n",
    "            test_indices = indices[mask]\n",
    "            train_indices = indices[~mask]\n",
    "            #initialize test set and training set\n",
    "            test_set = Dataset(ids=dataset.ids[test_indices], labels=dataset.labels[test_indices], data=dataset.data[test_indices])\n",
    "            train_set = Dataset(ids=dataset.ids[train_indices], labels=dataset.labels[train_indices], data=dataset.data[train_indices])\n",
    "            yield (train_set, test_set)\n",
    "        \n",
    "\n",
    "    #returns the best set of hyperparameters\n",
    "    def k_fold(self, dataset: Dataset, n_folds: int):\n",
    "        if(isinstance(dataset, Dataset)):\n",
    "            self._dataset = dataset\n",
    "        else:\n",
    "            raise TypeError\n",
    "        \n",
    "        if(type(n_folds) == int):\n",
    "            self._n_folds = n_folds\n",
    "        else:\n",
    "            raise TypeError\n",
    "        \n",
    "        data_size = dataset.ids.shape[0]\n",
    "        if(n_folds > data_size):\n",
    "            raise ValueError\n",
    "        \n",
    "        hyper_grid = self._hyper_grid\n",
    "        estimator = self._estimator\n",
    "\n",
    "        if dataset.data.ndim == 1:\n",
    "            input_dim = 1\n",
    "        else:\n",
    "            input_dim = dataset.data.shape[1]\n",
    "\n",
    "        if dataset.labels.ndim == 1:\n",
    "            output_dim = 1\n",
    "        else:\n",
    "            output_dim = dataset.labels.shape[1]\n",
    "\n",
    "        \n",
    "        #generates all combinations of hyperparameters\n",
    "        keys, values = zip(*hyper_grid.items())\n",
    "        param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "        #iterates all combinations of hyperparameters\n",
    "        for combination in param_combinations:\n",
    "            loss_keys = filter_dict_by_key(combination, self._loss_keys)\n",
    "            estimator_keys = filter_dict_by_key(combination, self._estimator_keys)\n",
    "            optimizer_keys = filter_dict_by_key(combination, self._optimizer_keys)\n",
    "            net_keys = filter_dict_by_key(combination, self._net_keys)\n",
    "            \n",
    "            estimator_params = {}\n",
    "\n",
    "            if loss_keys:\n",
    "                #TODO define new loss\n",
    "                pass\n",
    "            if optimizer_keys:\n",
    "                #TODO define new optimizer\n",
    "                pass\n",
    "            if net_keys:\n",
    "                #TODO define new net\n",
    "                pass\n",
    "            if estimator_keys:\n",
    "                #TODO update estimator\n",
    "                pass\n",
    "\n",
    "            #iterates folds of dataset\n",
    "            for train_set, test_set in self._generate_folds():\n",
    "                tmp = 1\n",
    "        \n",
    "        \n",
    "        #for loop iterating all combinations\n",
    "            #give combination to estimator to initialize new NN\n",
    "            #train on TR set\n",
    "            #evaluate on VL set\n",
    "            #save results for combination\n",
    "        #return best combination\n",
    "            \n",
    "            \n",
    "    #returns an estimation of the risk for the model, average +- variance\n",
    "    def nested_k_fold(dataset: Dataset, inner_n_folds:int, outer_n_folds:int):\n",
    "        print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork([\n",
    "    LinearLayer((8, 16)),\n",
    "    ActivationFunction(),\n",
    "    LinearLayer((16, 16)),\n",
    "    ActivationFunction(),\n",
    "    LinearLayer((16, 2))\n",
    "])\n",
    "estimator = Estimator(net)\n",
    "grid = {}\n",
    "grid['eta'] = [0.1, 0.2, 0.3]\n",
    "grid['momentum'] = [0.1, 0.2, 0.3]\n",
    "grid['layers'] = [[8,16,16,2], [7,10,2]]\n",
    "grid['pippo'] = ['paperino']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, [6, 1])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import read_monks\n",
    "data = read_monks(1, \"train\")\n",
    "ids = data.ids.copy()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following parameters were not accepted:  ['pippo'] .\n",
      " Only the following parameters are accepted ['layers', 'eta', 'l2', 'momentum', 'loss', 'batchsize', 'seed'] .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selector = GridSearch(estimator, grid)\n",
    "selector.k_fold(data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[7 5 1 4 3 6 2 0 9 8]\n"
     ]
    }
   ],
   "source": [
    "test = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "print(test)\n",
    "perm = np.random.permutation(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bd504499a3d325a7c1da9f8228712639636db49ae66a9009fa19a793144457f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
