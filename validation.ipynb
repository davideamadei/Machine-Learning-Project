{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class for Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO maybe change ValueError exceptions to show wrong value\n",
    "# TODO add comments to nested k-fold and check those in k-fold\n",
    "# TODO EarlyStopping docs\n",
    "\n",
    "# python libraries\n",
    "import numpy as np\n",
    "from typing import Iterator, Callable\n",
    "import itertools\n",
    "from numbers import Number\n",
    "\n",
    "# local libraries\n",
    "from ddnn.utils import Dataset\n",
    "from ddnn.nn import (\n",
    "    Estimator,\n",
    "    LossFunction,\n",
    "    Optimizer,\n",
    "    LinearLayer,\n",
    "    ActivationFunction,\n",
    "    NeuralNetwork,\n",
    ")\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(\n",
    "        self,\n",
    "        estimator: Estimator,\n",
    "        losses: list[str] = [\"MSE\"],\n",
    "        checks_to_stop: int = 10,\n",
    "        check_frequency: int = 10,\n",
    "    ) -> None:\n",
    "        self._estimator = estimator\n",
    "        self._losses = losses\n",
    "        self._n_worse_checks = 0\n",
    "        self._checks_to_stop = checks_to_stop\n",
    "        self._check_frequency = check_frequency\n",
    "        self._current_epoch = 0\n",
    "        self._current_best = dict.fromkeys(losses, float(\"inf\"))\n",
    "        self._best_epoch = 0\n",
    "\n",
    "    def __call__(self, record) -> None:\n",
    "        current_epoch = record[\"epoch\"]\n",
    "        self._current_epoch = current_epoch\n",
    "        if (current_epoch - 1) % self._check_frequency == 0:\n",
    "            validation_set = self._validation_set\n",
    "            estimator = self._estimator\n",
    "            losses = self._losses\n",
    "            validation_loss = self._estimator.evaluate(\n",
    "                losses=losses, dataset=validation_set\n",
    "            )\n",
    "            if validation_loss[losses[0]] < self._current_best[losses[0]]:\n",
    "                self._n_worse_checks = 0\n",
    "                self._current_best = validation_loss\n",
    "                self._best_epoch = current_epoch\n",
    "            else:\n",
    "                self._n_worse_checks += 1\n",
    "                if self._n_worse_checks == self._checks_to_stop:\n",
    "                    print(f\"Stopped early at epoch {current_epoch}.\")\n",
    "                    estimator.stop_training = True\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._n_worse_checks = 0\n",
    "        self._best_epoch = 0\n",
    "        self._current_best = dict.fromkeys(self._losses, float(\"inf\"))\n",
    "\n",
    "    def set_validation_set(self, validation_set: Dataset) -> None:\n",
    "        self.reset()\n",
    "        self._validation_set = validation_set\n",
    "\n",
    "\n",
    "class GridSearch:\n",
    "    _net_keys = [\"layers\"]\n",
    "    _optimizer_keys = [\"eta\", \"l2_coeff\", \"alpha\"]\n",
    "    _loss_keys = [\"fname\"]\n",
    "    _estimator_keys = [\"batchsize\"]\n",
    "    # _global_keys = _net_keys + _optimizer_keys + _loss_keys + _estimator_keys\n",
    "\n",
    "    # dictionary containing translations from exposed names to names to pass to functions internally\n",
    "    _param_name_translations = {\n",
    "        \"layers\": \"layers\",\n",
    "        \"l2\": \"l2_coeff\",\n",
    "        \"momentum\": \"alpha\",\n",
    "        \"eta\": \"eta\",\n",
    "        \"loss\": \"fname\",\n",
    "        \"batchsize\": \"batchsize\",\n",
    "    }\n",
    "\n",
    "    # check param_grid validity\n",
    "    @staticmethod\n",
    "    def _check_param_grid(param_dict: dict, hyper_grid: dict) -> None:\n",
    "        \"\"\"Checks that the given grid of hyperparameters is correct\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        param_dict : dict\n",
    "            dictionary of accepted hyperparameters\n",
    "        hyper_grid : dict\n",
    "            dictionary containing the grid of hyperparameters on which to perform the validity check\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            when an hyperparameter is missing or one of its possible values is not valid\n",
    "        TypeError\n",
    "            when an hyperparameter has a value with the wrong type or is not a list of values\n",
    "        \"\"\"\n",
    "\n",
    "        activation_functions = [\"ReLU\", \"logistic\", \"tanh\", \"linear\"]\n",
    "        loss_functions = [\"MSE\"]\n",
    "\n",
    "        for key in param_dict.keys():\n",
    "            if not key in hyper_grid.keys():\n",
    "                raise ValueError(\n",
    "                    (\n",
    "                        \"All the following parameters must be present in the\"\n",
    "                        \" hyperparameter grid: \"\n",
    "                    ),\n",
    "                    list(param_dict.keys()),\n",
    "                )\n",
    "            if not isinstance(hyper_grid[key], list) or len(hyper_grid[key]) == 0:\n",
    "                raise ValueError(\n",
    "                    \"Each parameter must have an associated not empty list of\"\n",
    "                    \" parameters\"\n",
    "                )\n",
    "\n",
    "        # check layers\n",
    "        for net in hyper_grid[\"layers\"]:\n",
    "            for layer in net:\n",
    "                if (\n",
    "                    not isinstance(layer, tuple)\n",
    "                    or not len(layer) == 2\n",
    "                    or not isinstance(layer[0], int)\n",
    "                    or not isinstance(layer[1], str)\n",
    "                ):\n",
    "                    raise TypeError(\n",
    "                        \"The layers parameter accepts a list of tuples of length 2 with\"\n",
    "                        \" the first element being an integer that is the number of\"\n",
    "                        \" units in that layer, and the second element is a string\"\n",
    "                        \" containing the name of the activation function for that layer\"\n",
    "                    )\n",
    "                if layer[0] <= 0:\n",
    "                    raise ValueError(\"The number of units must be greater than 0\")\n",
    "                if not layer[1] in activation_functions:\n",
    "                    raise ValueError(\n",
    "                        (\n",
    "                            \"Only the following values are accepted for activation\"\n",
    "                            \" function: \"\n",
    "                        ),\n",
    "                        activation_functions,\n",
    "                    )\n",
    "\n",
    "        # check l2\n",
    "        for l2_coeff in hyper_grid[\"l2\"]:\n",
    "            if not isinstance(l2_coeff, Number):\n",
    "                raise TypeError(\"The L2 coefficient must be a number\")\n",
    "            if l2_coeff < 0:\n",
    "                raise ValueError(\"The L2 coefficient must be at least 0\")\n",
    "\n",
    "        # check momentum\n",
    "        for momentum in hyper_grid[\"momentum\"]:\n",
    "            if not isinstance(momentum, Number):\n",
    "                raise TypeError(\"The momentum parameter must be a number\")\n",
    "            if momentum < 0:\n",
    "                raise ValueError(\"The momentum parameter must be at least 0\")\n",
    "\n",
    "        # check eta\n",
    "        for eta in hyper_grid[\"eta\"]:\n",
    "            if not isinstance(eta, Number):\n",
    "                raise TypeError(\"The learning rate must be a number\")\n",
    "            if eta <= 0:\n",
    "                raise ValueError(\"The learning rate must be greater than 0\")\n",
    "\n",
    "        # check loss functions\n",
    "        for loss in hyper_grid[\"loss\"]:\n",
    "            if not isinstance(loss, str):\n",
    "                raise TypeError(\n",
    "                    \"The loss must be a string corresponding to the required loss\"\n",
    "                )\n",
    "            if loss not in loss_functions:\n",
    "                raise ValueError(\n",
    "                    \"Only the following values are accepted for the loss: \",\n",
    "                    loss_functions,\n",
    "                )\n",
    "\n",
    "        # check batchsize\n",
    "        for batchsize in hyper_grid[\"batchsize\"]:\n",
    "            if not isinstance(batchsize, int):\n",
    "                raise TypeError(\"The batch size must be an integer\")\n",
    "            if batchsize <= 0 and batchsize != -1:\n",
    "                raise ValueError(\n",
    "                    \"The batch size must be greater than 0. If -1 is passed as a value\"\n",
    "                    \" the size of the dataset will be used\"\n",
    "                )\n",
    "\n",
    "    def __init__(self, estimator: Estimator, hyper_grid: dict):\n",
    "        \"\"\"Initializes a new instance\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        estimator : Estimator\n",
    "            the estimator to use for training and evaluation\n",
    "        hyper_grid : dict\n",
    "            grid of hyperparameters\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        TypeError\n",
    "            when parameter types are incorrect\n",
    "        \"\"\"\n",
    "        if estimator == None or type(estimator) != Estimator:\n",
    "            raise TypeError\n",
    "        self._estimator = estimator\n",
    "        if hyper_grid == None or type(hyper_grid) != dict:\n",
    "            raise TypeError\n",
    "\n",
    "        # check for wrong values\n",
    "        GridSearch._check_param_grid(self._param_name_translations, hyper_grid)\n",
    "\n",
    "        # translate names of parameters and sort by key for better efficiency\n",
    "        new_grid = {}\n",
    "        for key in self._param_name_translations.keys():\n",
    "            if key in hyper_grid:\n",
    "                new_grid[self._param_name_translations[key]] = hyper_grid[key]\n",
    "        self._hyper_grid = new_grid\n",
    "\n",
    "    # returns a list of data folds through indexes\n",
    "    def _generate_folds(\n",
    "        self, dataset: Dataset, n_folds: int\n",
    "    ) -> list[tuple[Dataset, Dataset]]:\n",
    "        \"\"\"function to generate the folds to use during grid search\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : Dataset\n",
    "            dataset to split in folds\n",
    "        n_folds : int\n",
    "            number of folds\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list(tuple[Dataset, Dataset])\n",
    "            returns a list containing tuples of Dataset classes. Each tuple is of the form (Training set, Test set)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        data_size = dataset.ids.shape[0]\n",
    "        indices = np.arange(data_size)\n",
    "\n",
    "        # TODO maybe shuffle not needed if we assume dataset has already been shuffled\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        folds = []\n",
    "\n",
    "        for index_lists in np.array_split(indices, n_folds):\n",
    "            # make mask to split test and training set indices\n",
    "            mask = np.zeros(data_size, dtype=bool)\n",
    "            mask[index_lists] = True\n",
    "            test_indices = indices[mask]\n",
    "            train_indices = indices[~mask]\n",
    "            # initialize test set and training set\n",
    "            test_set = Dataset(\n",
    "                ids=dataset.ids[test_indices],\n",
    "                labels=dataset.labels[test_indices],\n",
    "                data=dataset.data[test_indices],\n",
    "            )\n",
    "            train_set = Dataset(\n",
    "                ids=dataset.ids[train_indices],\n",
    "                labels=dataset.labels[train_indices],\n",
    "                data=dataset.data[train_indices],\n",
    "            )\n",
    "            folds.append((train_set, test_set))\n",
    "        return folds\n",
    "\n",
    "    def _create_estimator_params(self, combination: dict, input_dim: int) -> dict:\n",
    "        \"\"\"function to create the dictionary to pass to estimator for update\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        combination : dict\n",
    "            combination of hyperparameters to use\n",
    "        input_dim : int\n",
    "            number of features of dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            a dictionary to pass to the estimator's update function\n",
    "        \"\"\"\n",
    "        # filter parameters for various classes\n",
    "        loss_params = {key: combination[key] for key in self._loss_keys}\n",
    "        estimator_params = {key: combination[key] for key in self._estimator_keys}\n",
    "        optimizer_params = {key: combination[key] for key in self._optimizer_keys}\n",
    "        net_params = {key: combination[key] for key in self._net_keys}\n",
    "\n",
    "        # create dictionary of params to pass to constructors\n",
    "        estimator_params[\"loss\"] = LossFunction(**loss_params)\n",
    "        estimator_params[\"optimizer\"] = Optimizer(**optimizer_params)\n",
    "\n",
    "        # create list of layers to create NN\n",
    "        old_units = input_dim\n",
    "        layer_list = []\n",
    "        for layer in net_params[\"layers\"][:-1]:\n",
    "            layer_list.append(LinearLayer((old_units, layer[0])))\n",
    "            # TODO maybe linear layers can be removed\n",
    "            layer_list.append(ActivationFunction(fname=layer[1]))\n",
    "            old_units = layer[0]\n",
    "        last_layer = net_params[\"layers\"][-1]\n",
    "        layer_list.append(LinearLayer((old_units, last_layer[0])))\n",
    "        if last_layer[1] != \"linear\":\n",
    "            layer_list.append(ActivationFunction(fname=last_layer[1]))\n",
    "        estimator_params[\"net\"] = NeuralNetwork(layer_list)\n",
    "        return estimator_params\n",
    "\n",
    "    # returns the best set of hyperparameters\n",
    "    def k_fold(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        n_folds: int,\n",
    "        n_epochs: int,\n",
    "        callback: Callable[[dict], None] = print,\n",
    "        loss_list: list[str] = [\"MSE\"],\n",
    "        early_stopping: tuple[int, int] = None,\n",
    "    ) -> list:\n",
    "        \"\"\"function to execute a k-fold cross-validation on the given dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : Dataset\n",
    "            dataset to use for k-fold cross-validation\n",
    "        n_folds : int\n",
    "            number of folds to use in cross-validation\n",
    "        n_epochs : int\n",
    "            number of epochs to run training\n",
    "        callback : Callable[[dict], None], optional\n",
    "            callback function to use during training, by default print\n",
    "        loss_list: list[str]\n",
    "            list of loss functions to evaluate the test set on\n",
    "        early_stopping: tuple\n",
    "            dictionary containing two values, respectively how many checks have to fail before stopping training and how many epochs need to pass between checks\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            list containing results of the cross-validation ordered by increasing average loss on the test set.\n",
    "            Every element is a list of dictionaries containing the\n",
    "            combination of hyperparameters, the average of the test loss and the standard deviation on the test loss\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            when values of some parameters are incorrect\n",
    "        \"\"\"\n",
    "\n",
    "        hyper_grid = self._hyper_grid\n",
    "        estimator = self._estimator\n",
    "\n",
    "        data_size = dataset.shape[0]\n",
    "        input_dim = dataset.shape[1][0]\n",
    "        output_dim = dataset.shape[1][1]\n",
    "\n",
    "        # check n_folds value\n",
    "        if n_folds > data_size:\n",
    "            raise ValueError(\n",
    "                \"The number of folds cannot be greater than the number of samples in\"\n",
    "                \" the dataset\"\n",
    "            )\n",
    "        # check if output layer is correct for all combinations\n",
    "        for layers in hyper_grid[\"layers\"]:\n",
    "            if layers[-1][0] != output_dim:\n",
    "                raise ValueError(\n",
    "                    \"Number of units in last layer must be equal to the output\"\n",
    "                    \" dimension of the data\"\n",
    "                )\n",
    "        # check values for batchsize\n",
    "        for batchsize in hyper_grid[\"batchsize\"]:\n",
    "            if batchsize > data_size:\n",
    "                raise ValueError(\n",
    "                    \"The batchsize cannot be greater than the number of samples\"\n",
    "                )\n",
    "\n",
    "        # creates folds\n",
    "        folds = self._generate_folds(dataset=dataset, n_folds=n_folds)\n",
    "\n",
    "        # creates early stopping if it was passed to the function\n",
    "        if early_stopping != None:\n",
    "            early_stopper = EarlyStopping(\n",
    "                estimator=estimator,\n",
    "                losses=loss_list,\n",
    "                checks_to_stop=early_stopping[0],\n",
    "                check_frequency=early_stopping[1],\n",
    "            )\n",
    "\n",
    "        # generates all combinations of hyperparameters\n",
    "        keys, values = zip(*hyper_grid.items())\n",
    "        param_combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "        combination_results = []\n",
    "\n",
    "        # iterates all combinations of hyperparameters\n",
    "        for combination in param_combinations:\n",
    "\n",
    "            estimator_params = self._create_estimator_params(combination, input_dim)\n",
    "            if estimator_params[\"batchsize\"] == -1:\n",
    "                estimator_params[\"batchsize\"] = data_size\n",
    "            estimator.update_params(**estimator_params)\n",
    "\n",
    "            test_loss_list = []\n",
    "            epoch_list = []\n",
    "\n",
    "            print(combination)\n",
    "\n",
    "            # iterates folds of dataset\n",
    "            for train_set, test_set in folds:\n",
    "                if early_stopping != None:\n",
    "                    early_stopper.set_validation_set(test_set)\n",
    "\n",
    "                    def my_callback(record: dict) -> None:\n",
    "                        callback(record)\n",
    "                        early_stopper(record)\n",
    "\n",
    "                    estimator.train(\n",
    "                        dataset=train_set, n_epochs=n_epochs, callback=my_callback\n",
    "                    )\n",
    "                    epoch_list.append(early_stopper._best_epoch)\n",
    "                    test_loss_list.append(early_stopper._current_best)\n",
    "                else:\n",
    "                    estimator.train(\n",
    "                        dataset=train_set, n_epochs=n_epochs, callback=callback\n",
    "                    )\n",
    "                    test_loss_list.append(\n",
    "                        estimator.evaluate(losses=loss_list, dataset=test_set)\n",
    "                    )\n",
    "                estimator.reset()\n",
    "\n",
    "            test_loss_avg = {}\n",
    "            test_loss_std = {}\n",
    "            for loss in loss_list:\n",
    "                test_loss_avg[loss] = sum(d[loss] for d in test_loss_list) / len(\n",
    "                    test_loss_list\n",
    "                )\n",
    "                test_loss_std[loss] = np.std([d[loss] for d in test_loss_list])\n",
    "\n",
    "            # test_loss_avg = sum(test_loss_list) / n_folds\n",
    "            # test_loss_std = np.std(test_loss_list)\n",
    "\n",
    "            combination_results.append(\n",
    "                {\n",
    "                    \"parameters\": combination,\n",
    "                    \"test_loss_avg\": test_loss_avg,\n",
    "                    \"test_loss_std\": test_loss_std,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if early_stopping != None:\n",
    "                combination_results[-1][\"epoch_avg\"] = np.average(epoch_list)\n",
    "                combination_results[-1][\"epoch_std\"] = np.std(epoch_list)\n",
    "\n",
    "            print(combination_results)\n",
    "        if loss_list[0] == \"binary_accuracy\":\n",
    "            combination_results.sort(\n",
    "                key=lambda x: x[\"test_loss_avg\"][loss_list[0]], reverse=True\n",
    "            )\n",
    "        else:\n",
    "            combination_results.sort(key=lambda x: x[\"test_loss_avg\"][loss_list[0]])\n",
    "        return combination_results\n",
    "\n",
    "    # returns an estimation of the risk for the model, average +- standard deviation\n",
    "    def nested_k_fold(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        inner_n_folds: int,\n",
    "        outer_n_folds: int,\n",
    "        n_epochs: int,\n",
    "        inner_callback: Callable[[dict], None] = print,\n",
    "        outer_callback: Callable[[dict], None] = print,\n",
    "        loss_list: list[str] = [\"MSE\"],\n",
    "        early_stopping: tuple[int, int] = None,\n",
    "    ) -> dict:\n",
    "        \"\"\"function implementing nested k-fold cross validation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : Dataset\n",
    "            dataset to run cross validation on\n",
    "        inner_n_folds : int\n",
    "            number of folds for the inner cross validation\n",
    "        outer_n_folds : int\n",
    "            number of folds for the outer cross validation\n",
    "        n_epochs : int\n",
    "            number of epochs to run training for\n",
    "        inner_callback : Callable[[dict], None], optional\n",
    "            callback function to use during training for the inner cross validation, by default print\n",
    "        outer_callback : Callable[[dict], None], optional\n",
    "            callback function to use during training for the outer cross validation, by default print\n",
    "        loss_list: list[str]\n",
    "            list of loss functions to evaluate the test set on\n",
    "        early_stopping: dict\n",
    "            dictionary containing 'checks_to_stop' and 'check_frequency', respectively how many checks have to fail before stopping training and how many epochs need to pass between checks\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            a dictionary containing a list of tuples each containing the best combination of hyperparameters\n",
    "            on that fold and the corresponding loss on the test set for that fold, the average loss on the test sets across the folds\n",
    "            and their standard deviation\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            when values are incorrect\n",
    "        \"\"\"\n",
    "\n",
    "        estimator = self._estimator\n",
    "        data_size = dataset.shape[0]\n",
    "        folds = self._generate_folds(dataset=dataset, n_folds=outer_n_folds)\n",
    "        input_dim = dataset.shape[1][0]\n",
    "\n",
    "        # check outer_n_folds value\n",
    "        if outer_n_folds > data_size:\n",
    "            raise ValueError(\n",
    "                f\"The number of folds cannot be greater than the number of samples in\"\n",
    "                f\" the dataset: {outer_n_folds} > {data_size}\"\n",
    "            )\n",
    "\n",
    "        # check inner_n_folds value\n",
    "        if inner_n_folds > data_size:\n",
    "            raise ValueError(\n",
    "                f\"The number of folds cannot be greater than the number of samples in\"\n",
    "                f\" the dataset: {inner_n_folds} > {data_size}\"\n",
    "            )\n",
    "\n",
    "        test_loss_list = []\n",
    "        param_combination_list = []\n",
    "\n",
    "        for train_set, test_set in folds:\n",
    "            train_results = self.k_fold(\n",
    "                dataset=train_set,\n",
    "                n_folds=inner_n_folds,\n",
    "                n_epochs=n_epochs,\n",
    "                callback=inner_callback,\n",
    "                early_stopping=early_stopping,\n",
    "            )\n",
    "            params = train_results[0][\"parameters\"]\n",
    "            estimator_params = self._create_estimator_params(params, input_dim)\n",
    "            if estimator_params[\"batchsize\"] == -1:\n",
    "                estimator_params[\"batchsize\"] = data_size\n",
    "\n",
    "            estimator.update_params(**estimator_params)\n",
    "            if early_stopping != None:\n",
    "                early_epochs = int(train_results[0][\"epoch_avg\"])\n",
    "                estimator.train(\n",
    "                    dataset=train_set, n_epochs=early_epochs, callback=outer_callback\n",
    "                )\n",
    "            else:\n",
    "                estimator.train(\n",
    "                    dataset=train_set, n_epochs=n_epochs, callback=outer_callback\n",
    "                )\n",
    "            test_loss_list.append(\n",
    "                estimator.evaluate(losses=loss_list, dataset=test_set)\n",
    "            )\n",
    "            param_combination_list.append(params)\n",
    "\n",
    "        test_loss_avg = {}\n",
    "        test_loss_std = {}\n",
    "        for loss in loss_list:\n",
    "            test_loss_avg[loss] = sum(d[loss] for d in test_loss_list) / len(\n",
    "                test_loss_list\n",
    "            )\n",
    "            test_loss_std[loss] = np.std([d[loss] for d in test_loss_list])\n",
    "\n",
    "        results = {\n",
    "            \"test_loss_list\": list(zip(param_combination_list, test_loss_list)),\n",
    "            \"test_loss_avg\": test_loss_avg,\n",
    "            \"test_loss_std\": test_loss_std,\n",
    "        }\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NeuralNetwork(\n",
    "    [\n",
    "        LinearLayer((8, 16)),\n",
    "        ActivationFunction(),\n",
    "        LinearLayer((16, 16)),\n",
    "        ActivationFunction(),\n",
    "        LinearLayer((16, 2)),\n",
    "    ]\n",
    ")\n",
    "estimator = Estimator(net)\n",
    "grid = {}\n",
    "grid[\"eta\"] = [0.5, 0.6]\n",
    "grid[\"momentum\"] = [0.9]\n",
    "grid[\"layers\"] = [[(4, \"ReLU\"), (1, \"logistic\")]]\n",
    "grid[\"loss\"] = [\"MSE\"]\n",
    "grid[\"l2\"] = [1e-3, 1e-4, 1e-5]\n",
    "grid[\"batchsize\"] = [-1, 16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import read_monks, read_ML_cup, onehot_encoding\n",
    "\n",
    "# data = read_ML_cup(\"train\")\n",
    "data = read_monks(3, \"train\")\n",
    "data = onehot_encoding(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = GridSearch(estimator, grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.k_fold(\n",
    "    data, 5, n_epochs=400, loss_list=[\"MSE\", \"binary_accuracy\"], early_stopping=(10, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.nested_k_fold(\n",
    "    data, 5, 5, 10, loss_list=[\"MSE\", \"binary_accuracy\"], early_stopping=(1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5bd504499a3d325a7c1da9f8228712639636db49ae66a9009fa19a793144457f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
